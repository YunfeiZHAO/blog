<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GSOC week1 - Singularity - (May 24, 2021 - May 30, 2021)</title>
    <link href="/2021/05/30/GSOC1-singularity/"/>
    <url>/2021/05/30/GSOC1-singularity/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction-of-Singularity"><a href="#Introduction-of-Singularity" class="headerlink" title="Introduction of Singularity"></a>Introduction of Singularity</h1><p>During GSOC 2021 with Redhen, We have access to The High Performance Computing Clusters located at <a href="https://case.edu/utech/departments/research-computing">“CWRU”</a> that represents Case Western Reserve University. We need to have our final result run in a Singularity container on this HPC. I used Docker before, and this time I want to make an introduction to Singularity.<br>Before starting reading this article, I will recommand you with two resouces.<br><a href="https://github.com/hpcng/singularity">“Singularity Github”</a><br><a href="https://www.youtube.com/watch?v=vEjLuX0ClN0">“Introduction from San Diego Supercomputer Center”</a></p><ol><li><p>What is a container?<br>A (software) container is an abstraction for a set of technologies that aim to solve the problem of how to get software to run reliably when move from one computing environment to another.</p></li><li><p>What is Singularity?<br>Singularity is a kind of container for HPC (High Performance Computing Clusters).</p></li><li><p>What can Singularity do for you?<br>It’s a docker for HPC, that means it can do all the things that a container can do and you can let no trust users run untrusted containers. In the meanwhile, It supports HPC hardware and scientific applications.</p></li></ol><h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><p align="center"><a><img src="/img/singularity/shipping.png"style="width:600px;">Containers in digital world</a></p>Like what we ship goods with a container by ship, we put a huge variaty of goods in containers and ship them between warehouses, without container, it is less secured and will be a mess during the transport. As different servers or machines have different standards and different environment, we need to use container to make our applications run across different platform.<h2 id="Component-of-Singularity"><a href="#Component-of-Singularity" class="headerlink" title="Component of Singularity"></a>Component of Singularity</h2><ol><li><p>Container Image: A file (or collection of files) saved on disk that stores everything you need to run a taget application or applications: code, runtime, system tools, libraries, etc.</p></li><li><p>Container Process: A container process is simply a standard(Linux) process running on top of the underlying host’s operating system and kernel, but whose software environment is defined by the contents of the container image.</p></li></ol><h2 id="Compare-to-Virtual-Machines"><a href="#Compare-to-Virtual-Machines" class="headerlink" title="Compare to Virtual Machines"></a>Compare to Virtual Machines</h2><p>Container-based applications have direct access to the host kernel and hardware and, thus, are able to achieve similar performance to native applications. In contrast, VM-based applications only have indirect access via the guest OS and hypervisor, which creates a significant performance overhead.</p><p align="center"><a href="https://www.weave.works/blog/a-practical-guide-to-choosing-between-docker-containers-and-vms"><img src="/img/singularity/containers-vs-virtual-machines.jpg"style="width:600px;">Illustration of the structures of container and virtual machine. </a></p><h2 id="Compare-to-Docker"><a href="#Compare-to-Docker" class="headerlink" title="Compare to Docker"></a>Compare to Docker</h2><ol><li><p>HPC systems are shared resources and Docker is not designed for it.</p></li><li><p>Docker’s security model is designed to support trusted users running trusted containers(e.g. Users can escalate to root). </p></li><li><p>Docker is not designed to support tightly-coupled, highly distributed parallel applications(MPI).</p></li></ol><h2 id="Most-Common-Singularity-Use-Cases"><a href="#Most-Common-Singularity-Use-Cases" class="headerlink" title="Most Common Singularity Use Cases"></a>Most Common Singularity Use Cases</h2><ol><li><p>Building and running applications that require newer system libraries than those are available on host system.</p></li><li><p>Running commercial applications binairies that have specific OS requirements which are not met by host system.</p></li><li><p>Converting Docker containers to Singularity containers.</p></li></ol><h2 id="The-Singularity-Workflow"><a href="#The-Singularity-Workflow" class="headerlink" title="The Singularity Workflow"></a>The Singularity Workflow</h2><ol><li><p><strong>Build</strong> your Singularity containers on a local system where you have root or sudo access; e.g., a personal computer where you have installed Singularity.</p></li><li><p><strong>Transfer</strong> your Singularity containers to the HPC system where you wnat to run them.</p></li><li><p><strong>Run</strong> your Singularity containers on that HPC system.</p></li></ol><h1 id="Setup-A-singularity-on-HPC"><a href="#Setup-A-singularity-on-HPC" class="headerlink" title="Setup A singularity on HPC"></a>Setup A singularity on HPC</h1><p>It is recommanded to install the same version of Singularity used on the HPC system where you plan to run your containers. If you run on multiple HPC systems, then install the lowest version number you expect to use. It is better to use Ubuntu or Fedora-based local system. I chose to use Ubuntu 20.04 for this part.</p><p>Besides building a container there are also two interesting registry for Singularity.</p><ol><li><a href="https://cloud.sylabs.io/home">Sylabs.io HUB</a> (commercialised)</li><li><a href="http://datasets.datalad.org/?dir=/shub">Singularity HUB</a> (oringinal)</li></ol><p>They are like docker hub where you can build, share or download images.</p><h2 id="Useful-documents"><a href="#Useful-documents" class="headerlink" title="Useful documents"></a>Useful documents</h2><p>You may have a look at the detailed instructions to use <a href="https://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container">Singularity  Lab: Build A Container</a>.<br>There is also a very useful document <a href="https://sites.google.com/case.edu/techne-public-site/singularity">on Redhen about using Sigularity</a>.<br>There is also an introduction of using singularity on <a href="https://sites.google.com/a/case.edu/hpcc/hpc-cluster/software/Software-Guide/s-t/singularity">Case Western Reserve HPC</a>.<br>To install Singularity on your computer, have a look on <a href="https://sylabs.io/guides/3.3/user-guide/installation.html">the official documents of Singularity container</a>.</p><h2 id="Build-an-Openpose-singularity-container"><a href="#Build-an-Openpose-singularity-container" class="headerlink" title="Build an Openpose singularity container"></a>Build an Openpose singularity container</h2><ol><li><p>Write a singularity definition file we name as openpose.def here. I followed steps introduced by Frankier and Zhiqi who are GSOC2020 students. You can have a look at their <a href="https://github.com/frankier/openpose_containers">github</a>. My definition file can be found on <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/tree/main/singularity">My github</a>.</p></li><li><p>If you want to build Singularity container locally, you need to follow the <a href="https://singularity-tutorial.github.io/01-installation/">introduction here</a>. I chose the Sigularity version 2.5.2 which can aussi be used on my HPC server. If you want to build a container locally, you need to have enough space on your working space, it can easily take more than 1g of memory depends on your project.</p></li><li><p>To build a container, you need to have a root access and you can use this command to get into it.</p><ul><li><strong>sudo -i</strong><br>To run the build process, you need the command like this:</li><li><strong>sudo singularity build –sandbox openpose_container openpose.def</strong></li></ul></li><li><p>After the building process, you will find a new folder is created named openpose_container. You can shell into the container as follows (-w means “writable):</p><ul><li><strong>sudo singularity shell -w openpose_container</strong></li></ul></li><li><p>To create the final portable unchangeable image (after training etc.):</p><ul><li><strong>sudo singularity build openpose_container.simg openpose_container</strong></li></ul></li><li><p>Finally, you can either upload the whole folder of openpose_container or openpose_container.simg on your HPC. It is recommended by using <strong>rsync</strong> command than use <strong>scp</strong>. Because you need to upload some very big files only once. If the connection is interrupted, you can continue the work by reusing the same command.</p><ul><li><strong>sudo rsync –progress -a ./openpose_container/ server_domain_name:~/openpose_container/</strong></li></ul></li><li><p>If you want to a container to have a shared folder with your host, you can use the following command.</p><ul><li><strong>sudo singularity shell -w –bind ~/Desktop/container:/home  ubuntu_container</strong><br>In this way, you can bind the folder ~/Desktop/container on your host to /home on your container.</li></ul></li><li><p>To use a user in a container, but you do not in the sudoer group on HPC to change user. You can following these steps:</p><ul><li>Create a home folder locally and mount the home folder on a container home directory when run it with sudo.</li><li>Add a user with the same user name on HPC and switch to this user. Install everything like Anaconda, etc.</li><li>Upload the container(If it is already on the HPC, you do not need to do this) and the home folder on HPC. </li><li>Then, on HPC, you can run the container by binding the home directories like what you do locally, and you will find you are using the user in the container as the same name you have in HPC and the home directory is what you created locally.</li></ul></li><li><p>Load some necessary modules on HPC:</p><ul><li><strong>module load singularity</strong></li><li><strong>module load cuda</strong></li><li><strong>srun -p gpu --gpus 1 --mem 4000 --cpus-per-gpu 2 --pty bash # do -h to know more options</strong></li><li><strong>pestat -p gpu -w <hostname> # this will show memory usage, has to be run on another terminal</strong></li></ul></li><li><p>Use GPU in singularity:</p><ul><li>You can firstly have a look at this document from <a href="https://sylabs.io/guides/3.5/user-guide/gpu.html">Sylab.io</a></li><li>what I do is to use <strong>singularity shell –nv –bind ./home:/home –bind /usr/local/cuda-11.2:/usr/local/cuda ubuntu_container</strong><br>In this way you mount cuda directory and also a list of files in /usr/bin of the host to your conatainer:<br>nvidia-cua-mps-control dnvidia-cuda-mps-server nvidia-debugdump nvidia-persistenced nvidia-smi</li><li>You need to add cuda path in your .bashrc and source it<div class="code-wrapper"><pre><code class="hljs CUDA">export PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;binexport LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64</code></pre></div></li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>GSOC 2021 Redhen</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HPC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GSOC week0 - Homepage</title>
    <link href="/2021/05/23/GSOC0-acceptance/"/>
    <url>/2021/05/23/GSOC0-acceptance/</url>
    
    <content type="html"><![CDATA[<h1 id="GSOC-2021"><a href="#GSOC-2021" class="headerlink" title="GSOC 2021"></a>GSOC 2021</h1><p>I am very happy to be accepted this year as a GSOC student in the Redhen lab. Google Summer of Code is a global program focused on bringing more student developers into open source software development. Students work with an open-source organization on a 10-week programming project during their break from school. <a href="https://summerofcode.withgoogle.com/organizations/?sp-page=3">The organization list for 2021 is listed here</a> </p><h1 id="Redhen-Lab"><a href="#Redhen-Lab" class="headerlink" title="Redhen Lab"></a>Redhen Lab</h1><p>The International Distributed Little Red Hen Lab™ is a global big data science laboratory and cooperative for research into multimodal communication. The main idea of the Redhen lab is cooperation. It is not like a traditional university where professors lead students to do research, while, in the Redhen Lab everyone makes contributions to make progress of research in the multimodal communication world. <a href="https://www.redhenlab.org/home">The Redhen Lab home page</a> can be found here if you are interested in this community.</p><h1 id="Project-description"><a href="#Project-description" class="headerlink" title="Project description"></a>Project description</h1><p>Gesture recognition becomes popular in recent years since it can play an essential role in many fields, such as non-verbal communication, emotion analysis, human-computer interaction, etc.  We can notice that people use quite a lot of hand gestures in daily life. The research task is to detect hand gestures in raw news videos that are streams of RGB images. I propose a keypoints-based pose tracking system for human tracking and a Transformer and keypoints-based gesture detector for gesture detection to fulfill this task. This structure is composed of a keypoints extractor, a person tracker, and a gesture detector. So the mission has three main parts. The first part is to track people in temporal space.  In the second part, for each person, we use their hand keypoints features in temporal space to construct several keypoints sequences. The third part is to use these sequences to make predictions of the existence of gestures. I believe that for gesture detection tasks, both spatial and temporal information is important. So that is why we use the Transformer that can take into account the local shape information of hands in one frame and can also capture the global hand motion information across the frames. As hand gestures in news videos do not have a good definition of label class, we start with only detecting the existence of a hand gesture. The classification can be easily extended. The final evaluation will be done on Redhen’s “Newscape Dataset”.</p>]]></content>
    
    
    <categories>
      
      <category>GSOC 2021 Redhen</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Event</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DETR: DEtection TRansformer</title>
    <link href="/2021/04/04/DETR/"/>
    <url>/2021/04/04/DETR/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Before reading this article, you may have a look at my previous article <sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yunfei ZHAO “Transformer? Attention!”.">[2]</span></a></sup><a href="https://blog.yunfeizhao.com/2021/03/31/attention/">“Transformer? Attention!”</a> in which I explain “Attention mechanism” and “The Transformer”. Early in 2020, Facebook AI proposed a new way to use the Transformer in object detection task in paper <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a>  in which they present a new method that view object detection as a direct set prediction problem.  For now, it only has its basic version that means that it still has quite a lot of potential and can be optimized. However, it has already significantly outperformed competitive baselines for accuracy and run-time. </p><p>There are four special characteristics for this model:</p><ol><li>DETR predicts all objects at once and is trained end-to-end.</li><li>DETR does not have multiple hand-designed components that encode prior knowledge, like sliding windows, spatial anchors, non-maximal suppression, large set of proposals, or window centers.</li><li>DETR does not require any customized layers, and thus can be reproduced easily in any framework that conatins standard CNN and transformer classes.</li><li>DETR can be easily generalized to produce panoptic segmentation in a unified manner.</li></ol><p>Training code and pretrained models are available at <a href="https://github.com/facebookresearch/detr">“End-to-End Object Detection with Transformers”</a>.</p><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/detr_architecture.png" alt="DETR directly predicts (in parallel) the final set of detections by combining a common CNN with a transformer architecture. During training, bipartite matching uniquely assigns predictions with ground truth boxes. Prediction with no match should yield a “no object” (∅) class prediction." style="width:800px;"></p><p>The overall DETR architecture is surprisingly simple. It contains three main components.</p><ol><li>A CNN backbone to extract a compact feature representation.</li><li>An encoder-decoder transformer.</li><li>A simple feed forward network(FFN) that makes the final detection prediction.</li></ol><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p>Starting from the initial image $x_{img}\in R^{3\times H_0\times W_0}$(with 3 color channels) and All imput image are batched together, applying 0-padding adequately to ensure they all have the same dimensions ($H_0, W_0$) as the largest image of the batch. The conventional CNN backbone generats a lower-resolution activation map $f\in R^{C\times H\times W}$ as a <strong>set of image features</strong>. The dimensions of this feature map are $C = 2048$ and $H,W = \frac{H_0}{frac}, \frac{W_0}{frac}$ where <strong>frac</strong> is 32 for DETR and 16 for DETR DC5.</p><h2 id="Transformer-encoder"><a href="#Transformer-encoder" class="headerlink" title="Transformer encoder"></a>Transformer encoder</h2><p>Each encoder layer has a standard architecture (<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[3]</span></a></sup> <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a>)and consist of a multi-head self-attention module and a feed forward network (FFN). The encoder follows the following steps:</p><ol><li>Use a $1\times1$ convolutional kernal to reduces the channel dimension of the high-level activation map $f$ of dimension $R^{C\times H\times W} $ to a new feature map $z_0$ of dimension $R^{d\times H\times W}$ where $d$ is smaller than $C$.</li><li>Collapse $z_0$ into one dimension, resulting in a $d\times HW$ feature map which is the input sequence for the encoder.</li><li>The transformer architecture is permutation-invariant, we supplement it with fixed <span style="color:red"><strong>positional encodings</strong>  that are added to the input of <strong>each attention layer(how? in this case)</strong>.</span>.</li></ol><h2 id="Transformer-decoder"><a href="#Transformer-decoder" class="headerlink" title="Transformer decoder"></a>Transformer decoder</h2><p>The decoder follows the standard architecture of the transformer too, transforming $N$ embeddings of size $d$ using multi-headed self- and encoder-decoder attention mechanism.  To be noted that this model decodes the $N$ objects in parallel at each decoder layer(not autoregressive). Since the decoder is permutation-invariant, the N input embeddings must be different to produce different results. These input embeddings are <span style="color:red"><strong>learnt positional encodings</strong></span> that we refer to as <em><strong>Object Queries</strong></em>. These queries are added to the input of each attention layer similarly to the encoder.</p><p>It follows the following steps:</p><ol><li>The $N$ object queries are transformed into an output embedding by the decoder.</li><li>The output embeddings are independent decoded into box coordinates and class labels by a <strong>feed forward network</strong>, resulting N final predictions.</li></ol><p>Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using <strong>pair-wise relations between them</strong>, while being able to use <strong>the whole image as context</strong>.</p><h2 id="Prediction-feed-forward-networks-FFNs"><a href="#Prediction-feed-forward-networks-FFNs" class="headerlink" title="Prediction feed-forward networks(FFNs)"></a>Prediction feed-forward networks(FFNs)</h2><p>$$<br>FFN(x) = RELU(xW_1 + b_1)W_2 + b_2<br>$$</p><p>The final prediction is computed a 3-layer perceptron with RELU activation function and hidden dimension $d$, and a linear projection layer. Like YOLO, The FFN predicts the normalized center coordinates, height and weidth of the box w.r.t. the input image. It predicts also the class label using a softmax function. An additional special class label $\varnothing$ is introduced and is used to represent that no object is detected within a slot (The “background” class) as we predict a fixed-size set of N bounding box, where N is usually much larger than the actual number of objects of interest in an image.</p><h2 id="Auxiliary-decoding-losses"><a href="#Auxiliary-decoding-losses" class="headerlink" title="Auxiliary decoding losses"></a>Auxiliary decoding losses</h2><p>It is helpful to use auxiliary losses as  <sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rami Al-Rfou “Character-Level Language Modeling with Deeper Self-Attention”  AAAI 2019.">[4]</span></a></sup> <a href="https://ojs.aaai.org//index.php/AAAI/article/view/4182">“Character-Level Language Modeling with Deeper Self-Attention” </a> in decoder during training, espacially to help the model output the correct number of object of each class. The idea is to add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. An additional shared layer-norm is used to normalize the input to the prediction FFNs rom different decoder layer.</p><h2 id="Detailed-model-architecture"><a href="#Detailed-model-architecture" class="headerlink" title="Detailed model architecture"></a>Detailed model architecture</h2><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/detailed_detr_architecture.png" alt="DETR uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class and bounding box) or a “no object” class." style="width:800px;"></p><h3 id="Transformer-architecture"><a href="#Transformer-architecture" class="headerlink" title="Transformer architecture"></a>Transformer architecture</h3><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/transformer.png" alt="Architecture of DETR’s transformer.." style="width:800px;"></p><h1 id="Set-Prediction-and-Loss-function"><a href="#Set-Prediction-and-Loss-function" class="headerlink" title="Set Prediction and Loss function"></a>Set Prediction and Loss function</h1><p>As we mentioned at the beginning, this model views object detection as a direct set prediction problem.The basic set prediction task is mutilabel classification and is does not apply to problem such as detection where there is an underlying structure between elements(i.e., near-identical boxes which need post-processing like NMS to eliminate duplicates). So we designed a loss based on the Hungarian algorithm to find a biopartite matching between ground-truth and prediction. In this way, it enforces permutation-invariance, and guarantees that each target element has a unique match by making the model to learning the underlying structure between elements and makes the pipeline post-processing free. To ensure the performance, we use also <span style="color:red">parallel decoding</span> for the transformer.</p><p>The key element for training prossess is the <strong>loss function</strong>. DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger thatn the typical number of object in an image and we need to score predicted objects (<strong>class, position, size</strong>) with respect to the ground truth. So the loss function following two steps:</p><ol><li>Produce an optimal bipartite matching between predicted and ground truth objects.</li><li>Optimize object-specific (bounding box) losses.</li></ol><h2 id="Search-for-the-permutation"><a href="#Search-for-the-permutation" class="headerlink" title="Search for the permutation"></a>Search for the permutation</h2><p>This part, we will try to find the optimal permutation of $N$ predicted elements $\hat{\sigma} \in \mathcal{G}$. $y$ is the ground truth set of objects, and $\hat{y} = \{\hat{y}_i \}_{i=1}^{N}$.<br>Given that the size of y is smaller than $N$, it will be padded with $\varnothing$.</p><p>$$<br>\hat{\sigma} = \arg min_{\sigma \in \mathcal{G}_N} \sum_{i}^{N}\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)}),<br>$$</p><p>where $\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)})$ is a paire-wise <em>matching cost</em> between ground truth $y_i$ and a prediction with index $\sigma(i)$. This optimal assignment is computed efficiently with the Hungarian algorithm, following prior work (e.g. <sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Russell Stewart et al. “End-to-end people detection in crowded scenes” CVPR 2016">[5]</span></a></sup><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Stewart_End-To-End_People_Detection_CVPR_2016_paper.pdf">“End-to-end people detection in crowded scenes”</a>).</p><h3 id="Matching-cost"><a href="#Matching-cost" class="headerlink" title="Matching cost"></a>Matching cost</h3><p>The matching cost takes into account:</p><ol><li>The class prediction. We define $c_i$ as the target class label of the ith ground truth.</li><li>The similarity of predicted and ground truth boxes. We define $b_i \in  [0 ,1]^4$ as a vector that defines ground truth box center coordinates and its heigt and width relative to the image size.</li></ol><p>Each element i of the ground truth set can be seen as a $y_i = (c_i, b_i)$. For the prediction with the permutation $\sigma(i)$ we define probability of class $c_i$ as $\hat{p}_{\sigma(i)}(c_i)$ and the predicted box as $\hat{b}_{\sigma(i)}$. We define :</p><p>$$<br>\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)}) =<br>\underbrace{-\unicode{x1D7D9}_{\{c_i\neq\varnothing\}}\hat{p}_{\sigma(i)}(c_i)}_{\text{Higher probability of right class}} +<br>\underbrace{\unicode{x1D7D9}_{\{c_i\neq\varnothing\}}\mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)})}_{\text{Closer bounding box}}<br>$$<br>By using the permutation, we assume that we are finding <span style="color:red">one-to-one matching for direct set prediction without duplicates</span>.<br>In the matching cost we use probabilities $\hat{p}_{\hat{\sigma}(i)}(c_i)$ instead of log-probabilities. This makes the class prediction term commensurable to $\mathcal{L}_{box}(.,.)$. </p><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>After finding our optimal permutation $\hat{\sigma}$ in the last procedure, we compute the loss function in this step. The loss function is defined as a linear combination of a negative log-likelihood for class prediction and a box loss defined later:</p><p>$$<br>\mathcal{L}_{Hungarian}(y, \hat{y}) = \sum_{i=1}^{N}\left[<br>    -log\hat{p}_{\hat{\sigma}(i)}(c_i) +<br>    \unicode{x1D7D9}_{\{c_i\neq\varnothing\}}\mathcal{L}_{box}(b_i, \hat{b}_{\hat{\sigma}(i)})<br>    \right],<br>$$</p><p>To be noted that during the procedure of finding the permutation, we do not take the class label probability for $c_i = \varnothing$ into account, while for this loss function, we take it into account. In practice, we down-weight the log-probability term when $c_i = \varnothing$ by a factor 10 to account for class imbalance.</p><h2 id="Bounding-box-loss"><a href="#Bounding-box-loss" class="headerlink" title="Bounding box loss"></a>Bounding box loss</h2><p>To mitigate the scales problem of $l_1$ loss which induce the inconsistency between big object and little object, a linear combination of the &amp;l_1&amp; loss and the generalized $IoU$ loss is used.</p><p>$$<br>L_{box}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{iou}L_{iou}(b_i, \hat{b}_{\sigma(i)}) +<br>\lambda_{L1}||b_i - \hat{b}_{\sigma(i)}||_1<br>$$</p><p>where $\lambda_{iou}$, $\lambda_{L1} \in \mathcal{R}$ are hyperparameters. These two losses are normalized by the number of objects inside the batch.</p><h1 id="DETR-for-panoptic-segmentation"><a href="#DETR-for-panoptic-segmentation" class="headerlink" title="DETR for panoptic segmentation."></a>DETR for panoptic segmentation.</h1><p>DETR can be naturally extend by adding a mask head on top of the decoder outputs for panoptic segmentation. This head can be used to produce panoptic segmentation by treating stuff and thing classes in a unified way.</p><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/panoptic_head.png" alt="Illustration of the panoptic head. A binary mask is generated in parallel for each detected object, then the masks are merged using pixel-wise argmax." style="width:800px;"></p><p>To DETR panoptic segmentation has 4 parts:</p><ol><li>We train DETR to predict boxes around both $stuff$ and $things$ classes on COCO, using the same recipe. (Hungarian matching is computed using distance between boxes, so predicting boexs is required for the training to be possible)</li><li>A mask head predicts a binary mask for each of the predicted boxes. It takes as input the output of the transformer decoder for each object computes mutli-head(with M heads) attention scores of this embedding over the output of the encoder, generating M attention heatmaps per object in a small resolution.</li><li>An FPN-like architecture is used to make the final prediction and increase the resolution. The final resolution of the masks has stride 4 and each mask is supervised independently using the DICE/F-1 loss and Focal loss.</li><li>We predict the final panoptic segmentation we simply use an argmax over the mask scores at each pixel, and assign the corresponding categories to the resulting masks.</li></ol><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>DETR is a new design for object detection systems based on Transformer and bipartite matching loss from direct set prediction. It is one of the most popular model in 2020. Without bells and whistles, it achieves comparable results to a well optimized Faster R-CNN baseline on the challenging COCO dataset.<br>This new design for detectors also comes with new challenges, in particular regarding training, optimization and performances on small objects and it has large potential to be optimized.</p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><div class="code-wrapper"><pre><code class="hljs awk">@article&#123;zhao_DETR2021,     title=&#123;DETR: DEtection TRansformer&#125;,     url=&#123;https:<span class="hljs-regexp">//</span>blog.yunfeizhao.com<span class="hljs-regexp">/2021/</span><span class="hljs-number">04</span><span class="hljs-regexp">/04/</span>DETR/&#125;,     journal=&#123;https:<span class="hljs-regexp">//</span>blog.yunfeizhao.com/&#125;,     author=&#123;ZHAO, Yunfei&#125;, year=&#123;<span class="hljs-number">2021</span>&#125;, month=&#123;Mar&#125;&#125;</code></pre></div><iframe src="https://gitcdn.link/repo/YunfeiZHAO/blog/main/donation_unit/index.html" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Nicolas Carion, Francisco Massa et al. <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a>ECCV 2020.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Yunfei ZHAO <a href="https://blog.yunfeizhao.com/2021/03/31/attention/">“Transformer? Attention!”</a>.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Ashish Vaswani, et al. <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a> NIPS 2017.<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Rami Al-Rfou <a href="https://ojs.aaai.org//index.php/AAAI/article/view/4182">“Character-Level Language Modeling with Deeper Self-Attention”</a>  AAAI 2019.<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Russell Stewart et al. <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Stewart_End-To-End_People_Detection_CVPR_2016_paper.pdf">“End-to-end people detection in crowded scenes”</a> CVPR 2016<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Computer vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Paper reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer? Attention!</title>
    <link href="/2021/03/31/attention/"/>
    <url>/2021/03/31/attention/</url>
    
    <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Self-attention, it is a mechanism first used for nature language processing, such as language translation and text content summary,etc. Self-attention sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence, the sequence can be a phrase in NPL task.  At the time Google Brain released <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup><a href="https://arxiv.org/abs/1706.03762">“Attention is all you need.”</a>, this mechanism have already become an integral part of compelling sequence modeling and transduction models in various tasks. However such attention mechanism are used in conjunction with a recurrent network(RNN) or a convolutonal network(CNN). Given a sequence of words, if we see the word “eating”, then we will take more attention to the following name of food.</p><p align="center"><img src="/img/attention/sentence-example-attention.png" alt="Attention between words" style="width:400px;height:100px;"></p><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><p>One of the example of the application of this mechanism is the famous <sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ilya Sutskever, et al. “Sequence to Sequence Learning with Neural Networks”NIPS 2014.">[2]</span></a></sup><a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">“Sequence to Sequence Learning with Neural Networks”</a> model. Broadly speaking, it aims to transform an input sequence (source) to a new one (target) and both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages in either text or audio, and for tasks like adding video caption is also possible. The main struture Seq2seq model uses is the <strong>Encoder-Decoder</strong> structure.</p><p align="center"><img src="/img/attention/encoder-decoder.png" alt="RNN Encoder Decoder structure" style="width:400px"></p><ul><li><strong>Encoder</strong> processes the input sequence and compresses the information into a context vector of a fixed length. This vector is expected to be the encoded information or meaning of the input sequence.</li><li><strong>Decoder</strong> is initialized with the context vector to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state.<br>Both the encoder and decoder are recurrent neural networks, i.e. using LSTM or GRU units.</li></ul><p><strong>disadvantages</strong> While, there are some disadvantages of RNN for this task. Fixed-length context vector design is incapability of remembering long sentences. And for RNN, it has forgetten the fisrt parts once it completes processing the whole input. It also hard to parallel the whole processes, as the layers is run in sequence. </p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>People try to use CNN to solve the parallel issue.<br><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a></p><p align="center"><img src="/img/attention/CNN.png" alt="Using CNN to replace RNN and CNN can be parallel, Filters in higher layer canconsider longer sequence." style="width:300px"></p>But we have to build a lot more filter to have large receptive field.<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>The Transformer is a model proposed in <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup> based entirely on attention and almost all tasks use a RNN to perform attention mechanism can be repaced by the Transformer. The transformer as the new stat-of-art in 2020 outperforms the Google’s Neural Machine Translation System <sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yonghui Wu et al. “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”CoRR 2016">[5]</span></a></sup><a href="http://arxiv.org/abs/1609.08144">“GNMT”</a> in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering.</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>The Transformer also take the encode-decoder structure. A high level structure is showed below:<br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/The_transformer_encoders_decoders.png" alt="THe encode-decoder structure of the Transformer." style="width:400px"></p><p>The encoder maps an input sequence of sybol representations $x = (x_1,..,x_3)$ to a sequence of continous representations $z = (z_1,…,z_n)$. Given $z$, the decoder then generate an output sequence $y = (y_1,…,y_3)$ of symbols <strong>one element at a time</strong>. At each step the model is auto-regressive, <strong>consuming the previously generated symbols</strong> as additional input when generating the next. This will be represented in <strong>masking process</strong> in the decoder. </p><p align="center"><img src="/img/attention/embeddings.png" alt="Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes." style="width:400px"></p><p>While a more detailed model architecture is represented in “Attention is all you need”<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup> as below:</p><p align="center"><img src="/img/attention/model_architecture.png" alt="The Transformer - model architecture. The encoder and decoder shown in the left and right halves respectively. They both use stacked self-attention and point-wise, fully connected layers." style="width:400px"></p><ul><li><p><strong>The Encoder</strong> is composed of a tack of N=6 identical layers. For each layer, it has:</p><ol><li>A multi-head self-attention mechanism sub-layer.</li><li>A position-wise fully connected feed-forward network sub-layer.</li></ol></li><li><p><strong>The Decoder</strong> is also composed of a stack of N=6 identical layers. For each layer, it has:</p><ol><li>A multi-head self-attention mechanism sub-layer. The <strong>masking proceess</strong> I talk about before is added here too. Combined with fact that the output embeddings are offset by one position, ensure that the predictions for position $i$ can depend only on the known outputs at positions less than i.</li><li>A <strong>new</strong> multi-head self-attention machanism sub-layer, which performs attention over <strong>the output of the  encoder stack</strong>.</li><li>A position-wise fully connected feed-forward network sub-layer.</li></ol></li></ul><p>For each sub-layer, it has two operations:</p><ul><li>A residual connection, like in <sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="K. He, X. Zhang, S. Ren and J. Sun “Deep Residual Learning for Image Recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.">[7]</span></a></sup><a href="https://arxiv.org/abs/1512.03385">“Deep Residual Learning for Image Recognition”</a>.</li><li>A layer normalisation<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jimmy Lei Ba et al. “Layer Normalization”">[8]</span></a></sup><a href="https://arxiv.org/abs/1607.06450">“Layer Normalization”</a></li></ul><p>The output of each sub-layer is <strong>LayerNorm(x + Sublayer(x))</strong>, where Sublayer(x) is the function implemented by the sub-layer itself.</p><p>To facilitate the residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model}=512$</p><h2 id="A-detail-analyse-of-decoder"><a href="#A-detail-analyse-of-decoder" class="headerlink" title="A detail analyse of decoder"></a>A detail analyse of decoder</h2><h3 id="How-does-each-step-work"><a href="#How-does-each-step-work" class="headerlink" title="How does each step work"></a>How does each step work</h3><p>I was confused in the concept of a masking process showed in the first sub-layer in the decoder and how does the model is auto-regressive at each step. Let’s have a further look at the functionality of the decoder.</p><p>After finishing the encoding phase, the decoding phase begins. Each step in the decoding phase outputs an element from the output sequence. The number of steps depends on a <strong>special symbol which is generated</strong> and indicate that the decoder has completed its output. </p><p>At the begining, a defaut parameter element which indicate the begin of the output sequence is generate from the output of the encoder and then for each step, the elements from <strong>the previous generated sequence</strong> will be fed into the decoder and that is why we need a mask to let decoder to see only the output at position less than $i$ which is the indice of step (prediction for position $i$).  After the fisrt self-attention layer, the output of this layer will meet the hidden elements generated from the encoder(I will explain these hidden element later in the attention function) and go through the whole decoder stack. The mask process is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.<br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/transformer_decoding.gif" alt="Decoder functionality illustration" style="width:600px"></p><p><a href="https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0">“Illustrated Guide to Transformers- Step by Step Explanation”</a></p><p align="center"><img src="/img/attention/mask_score.png" alt="Adding a look-ahead mask to the scaled scores" style="width:400px"></p><h3 id="Final-linear-layer-and-Softmax-layer"><a href="#Final-linear-layer-and-Softmax-layer" class="headerlink" title="Final linear layer and Softmax layer"></a>Final linear layer and Softmax layer</h3><p>The decoder stack outputs a vector of floats and it is final linear layer and softmax layer to turn them into the sequence of worlds. </p><p>For example, the model knows 10,000 unique English world from its training dataset. The linear layer is then a simple fully connected neural network that takes input as decoder stack’s outputs vector and output a 10,000 dimension vector. The softmax layer turns the values in this vector into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.<br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/transformer_decoder_output_softmax.png" alt="This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word." style="width:600px"></p>For the training process, given the vectors of probabilities for the whole predicted sequence, we can then do the backpropagation for the whole model including the encoder and the decoder.<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>In the context of neural networks, <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention</a> is a technique that mimics cognitive attention. The effect enhances the important parts of the input data and fades out the rest – the thought being that the network should devote more computing power on that small but important part of the data. Which part of the data is more important than others depends on the context and is learned through training data by gradient descent.</p><p>The Transformer has it’s own attention mechanism called “Scaled Dot-Product Attention”. I’m going to introduce this model in the following</p><h2 id="Query-Key-Value"><a href="#Query-Key-Value" class="headerlink" title="Query, Key, Value"></a>Query, Key, Value</h2><p>The basic elements of this model is <strong>k: query</strong>, <strong>k: key</strong> of dimension $d_k$ and <strong>v: value</strong> of dimension $d_v$.<br>The matrix of attention is showed below:<br><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup><a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a><br>$$<br>Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>where, the compute attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. Let’s just keep this equation in mind for now.</p><p>The high level view of self-attention Layer is like this:<br><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a></p><p align="center"><img src="/img/attention/self_attention_view.png" alt="The high level view of self-attention model. We have a sequence of input x and a sequence of output b" style="width:300px"></p><p>Let’s have a  deeper view of “Scaled Dot-Product Attention”.</p><p align="center"><img src="/img/attention/attention_schema.png" alt="The schema to compute Scaled Dot-Product Attention and it's Matrix version presentation" style="width:800px"></p> <p>This is the schema to compute the scaled Dot-Product attention given an input sequence. People also often use additive attention that compute the compatibility function using a feed-forward network with a single hidden layer. While dot-product attention is much faster and more space-efficient in pratice. But the dot products grow large in magnitude. Assume that the components of $q$ and $k$ are independant random variable with mean 0 and vriance 1. Then their dot product, $q \cdot k = \sum^{d_k}_{i=1}$, has mean 0 and variance $d_k$. So we have the term $\sqrt{d_k}$ to scale the dot product.</p><h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p>From the paper of the Transformer, they found that it benecial to linearly project the queries, keys, and values h times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. It generate h $d_v$-dimensional output values in parallel and these are concatenated and once again projected, resulting in the final values.<br>$$<br>MultiHead(Q, K, V) = Concat(head_1, …, head_h)W^o<br>$$<br>where<br>$$<br>\text{head}_{i}  = \text{Attention}(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})<br>$$</p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><p><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a></p><p align="center"><img src="/img/attention/multi_head.png" alt="The schema of multi-head Scaled Dot-Product Attention" style="width:800px"></p><h1 id="Other-main-parts"><a href="#Other-main-parts" class="headerlink" title="Other main parts"></a>Other main parts</h1><p>From the Transformer <a href="/img/attention/model_architecture.png">model architecture schema</a>, we can notice that there are also several other components and I am going to do an introduction to these parts this section.</p><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>Position-wise Feed-Forward Networks is named <strong>Feed Forward</strong> in the model architecture schema and it is included in each layer of encoder and decoder. It is consists of two linear transformations with a RELU activation in between.<br>$$<br>FFN(x) = RELU(xW_1 + b_1)W_2 + b_2<br>$$<br>This function is applied to each position separately and identically, another way of describing this is as two convolutions with kernal size 1. While the linear transfermations are the same across different positions,they use different parameters from layer to layer(different $W$ and $b$).</p><h2 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h2><p>Embeddings are learned to convert the input tokens and output tokens to vectors of dimension $d_{model}$. Two embedding layer in encoder, decoder and the pre-softmax linear transformation at the output of the final decoder stack share the same weight matrix. They also multiply those weights by $\sqrt{d_{model}}$</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>We can notice that there is <strong>no positional information</strong> in self-attention as it is composed of dot product operations in parallel for each element of different position in the sequence. Some information about the relative or absolute position of the tokens in sequence need to be injected. The authors of The Transformer<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup> add <strong>“positional encodings”</strong> to the input embeddings at the bottoms of the encoder and decoder stacks.The positional encoding have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. While, there are many other choices of positional encodings, learned and fixed.</p><p>An intuitive way to understand the sum operation between vector of position encoding and the vector of embedded input showed below.<br><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a><br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/positional_embedding.png" alt="Positional embedding illustration" style="width:800px"></p>By summing these two terms, we integrate the one-hot vector which contains the current position information into the input.<h1 id="Why-Transformer-is-good"><a href="#Why-Transformer-is-good" class="headerlink" title="Why Transformer is good"></a>Why Transformer is good</h1><p>The reasons why Transformer becomes the stat-of-art in 2017 and has appealed attention in AI recently is listed below:</p><ol><li>The relative low computational complexity per layer.</li><li>The large amount of computation that can be parallelized.</li><li>The path length between long-range dependencies in the networl. As we can see that during the generation of Attention, an input element need to dot product with all other inputs and than generate weights and finally gets its attention element by the weighted average on the value of other input elements. It keeps the local and global information.</li></ol><h1 id="Some-interesting-images-from-the-paper"><a href="#Some-interesting-images-from-the-paper" class="headerlink" title="Some interesting images from the paper"></a>Some interesting images from the paper</h1><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup><a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a></p><p align="center"><img src="/img/attention/im1.png" style="width:600px"></p><p align="center"><img src="/img/attention/im2.png" style="width:600px"></p><p align="center"><img src="/img/attention/im3.png" style="width:600px"></p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><div class="code-wrapper"><pre><code class="hljs awk">@article&#123;zhao_Attention2021,     title=&#123;Transformer? Attention!&#125;,     url=&#123;https:<span class="hljs-regexp">//</span>blog.yunfeizhao.com<span class="hljs-regexp">/2021/</span><span class="hljs-number">03</span><span class="hljs-regexp">/31/</span>attention/&#125;,     journal=&#123;https:<span class="hljs-regexp">//</span>blog.yunfeizhao.com/&#125;,     author=&#123;ZHAO, Yunfei&#125;, year=&#123;<span class="hljs-number">2021</span>&#125;, month=&#123;Mar&#125;&#125;</code></pre></div><iframe src="https://gitcdn.link/repo/YunfeiZHAO/blog/main/donation_unit/index.html" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Ashish Vaswani, et al. <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a> NIPS 2017.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Ilya Sutskever, et al. <a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">“Sequence to Sequence Learning with Neural Networks”</a>NIPS 2014.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Hung-yi Lee. <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Nicolas Carion, Francisco Massa et al. <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a>ECCV 2020.<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Yonghui Wu et al. <a href="http://arxiv.org/abs/1609.08144">“Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”</a>CoRR 2016<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Jay Alammar<a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a><a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>K. He, X. Zhang, S. Ren and J. Sun <a href="https://arxiv.org/abs/1512.03385">“Deep Residual Learning for Image Recognition,”</a> 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.<a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Jimmy Lei Ba et al. <a href="https://arxiv.org/abs/1607.06450">“Layer Normalization”</a><a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Michael Phi <a href="https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522b">“Illustrated Guide to Transformers- Step by Step Explanation”</a><a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Computer vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Paper reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Some basic sorting algorithms</title>
    <link href="/2020/08/29/sorting-algorithms/"/>
    <url>/2020/08/29/sorting-algorithms/</url>
    
    <content type="html"><![CDATA[<iframe src="https://gitcdn.link/repo/YunfeiZHAO/blog/main/donation_unit/index.html" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    
    <tags>
      
      <tag>Competitive programming</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
