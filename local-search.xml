<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Selenium</title>
    <link href="/2024/01/26/info/selenium/"/>
    <url>/2024/01/26/info/selenium/</url>
    
    <content type="html"><![CDATA[<p>Selenium is a project for browser automation and testing. It includes three main parts.</p><ul><li>Integrated Development Environment</li><li>Selenium WebDriver: Tool for automating web browser interactions and this is what we often use for web scraping.</li><li>Selenium Grid: a program used to concurrently run test cases across multiple browsers, computers, and operating systems.</li></ul><p>All the work will be done on a remote ubuntu 22 server (you can do it locally or in a VM).</p><h1 id="Setup-python-package-and-drivers"><a href="#Setup-python-package-and-drivers" class="headerlink" title="Setup python package and drivers"></a>Setup python package and drivers</h1><div class="code-wrapper"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># Chrome dependencies</span>sudo apt-<span class="hljs-built_in">get</span> install libxss1 libappindicator1 libindicator7<span class="hljs-comment"># selenium python package</span>pip install selenium<span class="hljs-comment"># chromium browser driver</span>sudo apt-<span class="hljs-built_in">get</span> install chromium-chromedriver<span class="hljs-comment"># a library that provides a configuration database system used by some applications on Linux systems</span>sudo apt-<span class="hljs-built_in">get</span> install libgconf<span class="hljs-number">-2</span><span class="hljs-number">-4</span><span class="hljs-comment"># download and install chrome on remote server</span>wget <span class="hljs-keyword">https</span>://dl.google.com/linux/direct/google-chrome-stable_current_amd64.debsudo chmod +x google-chrome-stable_current_amd64.debsudo apt-<span class="hljs-built_in">get</span> install -y ./google-chrome-stable_current_amd64.debgoogle-chrome <span class="hljs-comment">--version</span></code></pre></div><h1 id="Setup-X11-for-remote-connection"><a href="#Setup-X11-for-remote-connection" class="headerlink" title="Setup X11 for remote connection"></a>Setup X11 for remote connection</h1><p>If your local machine is linux distribution with X11, it is much easier for setup. If you are using a windows local machine, you can follow my steps.</p><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># Remote server</span><span class="hljs-built_in">sudo</span> apt install xorg xauth x11-xserver-utils<span class="hljs-comment"># setup ssh configuration file</span><span class="hljs-built_in">sudo</span> vim /etc/ssh/sshd_config<span class="hljs-comment"># modify these lines</span>X11Forwarding <span class="hljs-built_in">yes</span>X11DisplayOffset 10<span class="hljs-comment"># restart ssh server for remote linux</span><span class="hljs-built_in">sudo</span> systemctl restart ssh</code></pre></div><p>You need to install an X server on Windows so that the APP GUI on the remote server can be redirected to the Win server. I recommendmand <em>vcxsrv</em> as it is stable. Once it is run, a little icon of X11 is shown in the hidden icons bar.<br>The <strong>network setting</strong> on the local server network profile type needed to be set as a private network so that the local device is discoverable on the network. Otherwise, the redirected X11 APP can not find the X server on your local machine.</p><div class="code-wrapper"><pre><code class="hljs nginx"><span class="hljs-comment"># local</span><span class="hljs-comment"># Establisha ssh connection with -X. </span><span class="hljs-comment"># It allows graphical applications running on </span><span class="hljs-comment"># the remote server to be displayed locally.</span><span class="hljs-attribute">ssh</span> -X username<span class="hljs-variable">@remote</span>-server-ip</code></pre></div><div class="code-wrapper"><pre><code class="hljs routeros"><span class="hljs-comment"># remote </span><span class="hljs-built_in">export</span> <span class="hljs-attribute">DISPLAY</span>=orz_win.local:0</code></pre></div><p>This environment variable tells the X11 where to redirect the APP. Where orz_win.local is the domain name of my local machine on my router. You can also use the local internet ip address. The 0 is the display number that you set when run a X server on your local machine. So you can open multiple X server on local machine and show different programs run on the remote server.</p><p>Now if you run google chrome on remote server by this command:</p><div class="code-wrapper"><pre><code class="hljs ebnf"><span class="hljs-attribute">google-chrome</span></code></pre></div><p>You will see that a chrome window is opened on your local computer.</p><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    <categories>
      
      <category>Informatics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>data</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Setup a remote linux server, DDNS,  MSI bios update</title>
    <link href="/2023/04/29/info/set-up-linux/"/>
    <url>/2023/04/29/info/set-up-linux/</url>
    
    <content type="html"><![CDATA[<p>I am writing this blog to record a bad day with my new PC.</p><h1 id="Hardware-part"><a href="#Hardware-part" class="headerlink" title="Hardware part"></a>Hardware part</h1><p>I have an MSI b550m mortar as a motherboard and a Ryzen 5900x as a CPU. But this motherboard bios do not support this CPU. So I need to replace it with my old Ryzen 1700x and update the bios by downloading the file from MSI’s official site and putting it in a USB root to update the motherboard from bios.</p><p>I have also accidentally removed my Win11 System EFI partition, so neither the GRUB on Ubuntu nor the boot manager from bios can wake my win11 up as it no longer possesses a boat loader.</p><p>I use a win11 iso file to enter the recovery mode and use the following command to recover this partition.</p><ol><li><p>Boot your computer using the installation media or recovery disk. On the first screen.</p><div class="code-wrapper"><pre><code class="hljs asciidoc"><span class="hljs-bullet">- </span>Enter the Command Prompt</code></pre></div></li><li><p>Run the commands below to shrink a partition for<br>unallocated space:</p><div class="code-wrapper"><pre><code class="hljs pgsql">- diskpart- list disk- <span class="hljs-keyword">select</span> disk # ( <span class="hljs-keyword">Select</span> the disk <span class="hljs-keyword">where</span> you want <span class="hljs-keyword">to</span> <span class="hljs-keyword">add</span> the EFI <span class="hljs-keyword">system</span> <span class="hljs-keyword">partition</span>.)- list <span class="hljs-keyword">partition</span>- <span class="hljs-keyword">select</span> <span class="hljs-keyword">partition</span> # (<span class="hljs-keyword">Select</span> the <span class="hljs-keyword">partition</span> which you plan <span class="hljs-keyword">to</span> shrink, <span class="hljs-keyword">or</span> the place you want <span class="hljs-keyword">to</span> <span class="hljs-keyword">create</span> the efi <span class="hljs-keyword">partition</span>)- shrink desired=<span class="hljs-number">100</span> (Shrink the selected <span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-number">100</span>MB, you can give more <span class="hljs-keyword">if</span> you want. <span class="hljs-keyword">If</span> you have unallocated <span class="hljs-keyword">partition</span>, you <span class="hljs-keyword">do</span> <span class="hljs-keyword">not</span> need this step)</code></pre></div></li><li><p>Run the following commands to create the EFI system partition with the unallocated space.</p><div class="code-wrapper"><pre><code class="hljs awk">- create partition efi size=<span class="hljs-number">100</span> (allocate the unused partition)- format quick fs=fat32 (EFI partition can only be fat32)- assign letter=S (You may replace “S” with other letters which are not already used.) - <span class="hljs-keyword">exit</span></code></pre></div></li><li><p>Use the command below to copy the boot files from the Windows partition to the EFI system partition and create the BCD store in it:</p><div class="code-wrapper"><pre><code class="hljs bash">bcdboot C:\windows /s S:</code></pre></div><p>“C” is the drive letter of the system partition and “S” is the drive letter you assign to the EFI partition. But you need to check from diskpart, as your windows main partition may not on C: if you have serveral disk</p></li></ol><h1 id="Internet-Setup"><a href="#Internet-Setup" class="headerlink" title="Internet Setup"></a>Internet Setup</h1><h2 id="What-is-a-Dynamic-DNS"><a href="#What-is-a-Dynamic-DNS" class="headerlink" title="What is a Dynamic DNS?"></a>What is a Dynamic DNS?</h2><p>Dynamic DNS allows you to direct your domain a resource that has dynamically assigned IP address. You need to creat an A (for IPv4) or AAAA (for IPv6) record on your DNS server.</p><p align="center"><img src="/img/set-up-linux/DDNS.jpg" alt="Set up DDNS on Google domain" style="width:400px"></p><h2 id="How-to-update-DDNS"><a href="#How-to-update-DDNS" class="headerlink" title="How to update DDNS"></a>How to update DDNS</h2><p>As we know that the public IP address attributed by our internet operator may be changed time to time, so we need to set a client program on our host, server, or gateway that does the following:</p><ul><li>Detects IP address changes</li><li>Uses the generated username and password</li><li>Communicates the new address to the Google name servers</li></ul><p>Google domain provides an API to perform manual updates with the API by making a POST request ot GET to the following URL:</p><div class="code-wrapper"><pre><code class="hljs vim">domains.google.<span class="hljs-keyword">com</span>/nic/<span class="hljs-keyword">update</span></code></pre></div><p>The API requires HTTPS and here is an example request:</p><div class="code-wrapper"><pre><code class="hljs vim">https://username:password@domains.google.<span class="hljs-keyword">com</span>/nic/<span class="hljs-keyword">update</span>?<span class="hljs-built_in">hostname</span>=subdomain.yourdomain.<span class="hljs-keyword">com</span>&amp;myip=<span class="hljs-number">1.2</span>.<span class="hljs-number">3.4</span></code></pre></div><p>The <strong>username</strong> and <strong>password</strong> correspond to the dynamic DNS you’ve created. The <strong>hostname</strong> is the domain name you want to attribute to your IP address. Finally, you need to detect your server IP address regularly. Once you notice your IP address is changed, you can use this API to change your DDNS table.</p><h2 id="Send-request-by-bash"><a href="#Send-request-by-bash" class="headerlink" title="Send request by bash"></a>Send request by bash</h2><p>We use <strong>curl</strong> command to make an https request to google. </p><div class="code-wrapper"><pre><code class="hljs routeros"><span class="hljs-attribute">GOOGLE_DDNS_USERNAME</span>=<span class="hljs-string">&quot;your_google_ddns_username&quot;</span><span class="hljs-attribute">GOOGLE_DDNS_PASSWORD</span>=<span class="hljs-string">&quot;your_google_ddns_password&quot;</span><span class="hljs-attribute">GOOGLE_DDNS_DOMAIN</span>=<span class="hljs-string">&quot;your.domain.com&quot;</span><span class="hljs-attribute">ip</span>=<span class="hljs-string">&quot;&quot;</span><span class="hljs-attribute">URL</span>=<span class="hljs-string">&quot;https://<span class="hljs-variable">$GOOGLE_DDNS_USERNAME</span>:<span class="hljs-variable">$GOOGLE_DDNS_PASSWORD</span>@domains.google.com/nic/update?hostname=<span class="hljs-variable">$GOOGLE_DDNS_DOMAIN</span>&amp;myip=<span class="hljs-variable">$ip</span>&quot;</span><span class="hljs-comment"># update the record</span>curl -s <span class="hljs-variable">$URL</span></code></pre></div><h2 id="Detect-current-IP-address"><a href="#Detect-current-IP-address" class="headerlink" title="Detect current IP address"></a>Detect current IP address</h2><p>To get our public IP address, we need to make request to external server.<br>The API I use is:</p><div class="code-wrapper"><pre><code class="hljs 1c">curl <span class="hljs-punctuation">-</span>s https<span class="hljs-punctuation">:</span><span class="hljs-comment">//ipinfo.io/ip</span></code></pre></div><h2 id="Do-not-disturb-web-servers"><a href="#Do-not-disturb-web-servers" class="headerlink" title="Do not disturb web servers"></a>Do not disturb web servers</h2><p>Now that we know the current IP address, we need to set an appropriate frequency to request the <em>ipinfo.io</em>  and when we detect a change, we update our DDNS. The complete code is below.</p><p>google-ddns.sh</p><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>GOOGLE_DDNS_USERNAME=<span class="hljs-string">&quot;&quot;</span>GOOGLE_DDNS_PASSWORD=<span class="hljs-string">&quot;&quot;</span>GOOGLE_DDNS_DOMAIN=<span class="hljs-string">&quot;&quot;</span><span class="hljs-comment"># update up address</span>ip=$(curl -s https://ipinfo.io/ip) FILE=./ipv4.txtold_ip=<span class="hljs-string">&quot;&quot;</span><span class="hljs-keyword">if</span> <span class="hljs-built_in">test</span> -f <span class="hljs-string">&quot;<span class="hljs-variable">$FILE</span>&quot;</span>; <span class="hljs-keyword">then</span>    old_ip=$(<span class="hljs-built_in">cat</span> ipv4.txt)<span class="hljs-keyword">fi</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$old_ip</span>&quot;</span> != <span class="hljs-string">&quot;<span class="hljs-variable">$ip</span>&quot;</span> ]; <span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$ip</span> &gt; ./ipv4.txt    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$ip</span>    <span class="hljs-comment"># update the record</span>    URL=<span class="hljs-string">&quot;https://<span class="hljs-variable">$GOOGLE_DDNS_USERNAME</span>:<span class="hljs-variable">$GOOGLE_DDNS_PASSWORD</span>@domains.google.com/nic/update?hostname=<span class="hljs-variable">$GOOGLE_DDNS_DOMAIN</span>&amp;myip=<span class="hljs-variable">$ip</span>&quot;</span>    curl -s <span class="hljs-variable">$URL</span><span class="hljs-keyword">fi</span></code></pre></div><h2 id="Launch-the-command-every-30mins"><a href="#Launch-the-command-every-30mins" class="headerlink" title="Launch the command every 30mins"></a>Launch the command every 30mins</h2><p>We need to run the script above every 30 minutes to ensure that even if our public IP changes, we have to wait only 30 minutes to regain access to it.</p><p>I chose to use Systemd timers. Systemd is a software suite that provides an array of system components for Linux operating systems. The main aim is to unify service configuration and behaviour across Linux distributions.</p><p>systemd timer units provide a mechanism for scheduling jobs on Linux. The execution time of these jobs can be based on the time and date or on events. There is a google tutorial from <a href="https://documentation.suse.com/smart/systems-management/html/systemd-working-with-timers/">“SUSE”</a></p><p>There are already many systemd timers that are part of system maintenance procedures that happen in the background of any Linux host. You can see then by command:</p><div class="code-wrapper"><pre><code class="hljs maxima">systemctl <span class="hljs-built_in">status</span> *<span class="hljs-built_in">timer</span></code></pre></div><p>In a nutshell, we need to create service and timer configuration files.</p><ul><li>google_ddns.service</li><li>google_ddns.timer</li></ul><p>On ubuntu these files are located in </p><div class="code-wrapper"><pre><code class="hljs awk"><span class="hljs-regexp">/lib/</span>systemd<span class="hljs-regexp">/system/</span></code></pre></div><p>We add following files in this folder.</p><p>google_ddns.service</p><div class="code-wrapper"><pre><code class="hljs ini"><span class="hljs-section">[Unit]</span><span class="hljs-attr">Description</span>=<span class="hljs-string">&quot;Run google DDNS updater script&quot;</span><span class="hljs-section">[Service]</span><span class="hljs-attr">ExecStart</span>=/your_path/google-ddns.sh</code></pre></div><p>google_ddns.timer</p><div class="code-wrapper"><pre><code class="hljs ini"><span class="hljs-section">[Unit]</span><span class="hljs-attr">Description</span>=<span class="hljs-string">&quot;Run google_ddns.service 5min after boot and every 30mins relative to activation time&quot;</span><span class="hljs-section">[Timer]</span><span class="hljs-attr">OnBootSec</span>=<span class="hljs-number">5</span>min<span class="hljs-attr">OnUnitActiveSec</span>=<span class="hljs-number">30</span>min<span class="hljs-attr">Unit</span>=google_ddns.service<span class="hljs-section">[Install]</span><span class="hljs-attr">WantedBy</span>=multi-user.target</code></pre></div><h2 id="Manage-timers-using-the-systemctl-command"><a href="#Manage-timers-using-the-systemctl-command" class="headerlink" title="Manage timers using the systemctl command"></a>Manage timers using the systemctl command</h2><div class="code-wrapper"><pre><code class="hljs maxima">systemctl <span class="hljs-built_in">status</span> google_ddns.<span class="hljs-built_in">timer</span>sudo systemctl start google_ddns.<span class="hljs-built_in">timer</span>sudo systemctl <span class="hljs-built_in">restart</span> google_ddns.<span class="hljs-built_in">timer</span>sudo systemctl stop google_ddns.<span class="hljs-built_in">timer</span></code></pre></div><p>Once you start the timer, you will get something like this by showing the timer status.</p><p align="center"><img src="/img/set-up-linux/systemd.jpg" alt="Sytemd timer status" style="width:500px"></p><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    <categories>
      
      <category>Informatics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Remote Server</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Final Report GSOC2021</title>
    <link href="/2021/08/22/cs/GSOC-final-2021/"/>
    <url>/2021/08/22/cs/GSOC-final-2021/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction-of-the-project"><a href="#Introduction-of-the-project" class="headerlink" title="Introduction of the project"></a>Introduction of the project</h1><p>The subject of this project is “Gesture temporal detection pipeline for news videos”. The gesture temporal detection in this project is a mission for detecting human hand gestures in a video. For now, our objective is to locate gestures in temporal space. For each frame of a video, we need to predict whether it is included in an action of hand gestures from persons in it. As non-verbal communication plays an important role in our daily life, a detector like this can have a meaningful use for researchers, especially for linguists. Because it is a complicated task for a human to extract gestures from videos of high volume. With this kind of detector, it can help researchers to extract gestures and then do further analyses. Besides, it could be also useful to help researchers to understand the correlation between a hand gesture and human body movement.<br>The code of this project can be found on my github.<br>The project link: <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021">https://github.com/YunfeiZHAO/gsoc-redhen-2021</a></p><h1 id="State-of-the-project"><a href="#State-of-the-project" class="headerlink" title="State of the project"></a>State of the project</h1><p>After two months of work, I finished the first version of this project. In this version, the video that we can deal with has to consent to the following conditions:</p><ol><li>The length of the video needs to be around 10 seconds with around 300 frames because I trained the model on the Chalearn hand gesture dataset and all of the data for training are small pieces of videos. Furthermore, the model is built based on Transformer, if the video is long, it will calculate attention for keypoints from unrelated timestamps and it will slow down the system. If it is a long video, we need to split the frames into groups to generate predictions this procedure will be added.</li><li>The video needs to have an acceptable resolution. The project used Openpose to extract keypoints from video. If the resolution of a video is too low, it will fail to extract keypoints.</li><li>The video can have only one person in a scene because I haven’t added a model to decompose multi-person scenes into single-person keypoints. But this part could be easily added.</li></ol><h1 id="Pipeline-for-gesture-temporal-detection"><a href="#Pipeline-for-gesture-temporal-detection" class="headerlink" title="Pipeline for gesture temporal detection"></a>Pipeline for gesture temporal detection</h1><p>Our pipeline is composed of three parts.</p><h2 id="A-video-processing-script"><a href="#A-video-processing-script" class="headerlink" title="A video processing script"></a>A video processing script</h2><p>The video processing script located in <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/blob/main/gesture_detector/video.py">video.py</a>. In this part, I created some tools for video and frames convertion and also a function for keypoints visualisation and a frames selection function. In video processing, we mainly generate the final frames of video which will be used later for Openpose to extract keypoints.</p><h2 id="A-singularity-container-which-is-epuiped-with-Openpose"><a href="#A-singularity-container-which-is-epuiped-with-Openpose" class="headerlink" title="A singularity container which is epuiped with Openpose"></a>A singularity container which is epuiped with Openpose</h2><p>The definition file of this container can be found in <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/tree/main/singularity">openpose.def</a> and you can use this file to generate a singuarity container that contains Openpose directly. When you have this container, you can log in with a user named ‘yxz2560’ (my account name on HPC) and mount home with a structure as following:</p><div class="code-wrapper"><pre><code class="hljs 1c">home  │<span class="hljs-string">|───yxz2569</span>    │    │───ellen_show    │   <span class="hljs-string">|───video</span>    │   <span class="hljs-string">|───frames</span>    │   <span class="hljs-string">|───keypoints</span>    │    <span class="hljs-string">|───gsoc-redhen-2021</span></code></pre></div><p>Normally, I put a video in <strong>video</strong> folder and generate frames in <strong>frames</strong> folder then the frames with keypoints and the keypoint’s json file is generated in <strong>keypoints</strong> folder. There are also some sub-folders in these three folders, and they are automatically created by my scripts. </p><p>Then in the container, you can run <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/blob/main/gesture_detector/keypoints.py">keypoints.py</a> to generate frames with keypoints for visualisation and a json file which contents the necessaire keypoints information.</p><h2 id="A-gesture-detector-based-on-DETR"><a href="#A-gesture-detector-based-on-DETR" class="headerlink" title="A gesture detector based on DETR"></a>A gesture detector based on DETR</h2><p>DETR is a model for object detection, and I wrote a post to introduce this <a href="/2021/04/04/DETR/">model</a>. The whole model and the data loader are modified to adapt to our problem. The source code of the model can be found here <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/tree/main/gesture_detector">https://github.com/YunfeiZHAO/gsoc-redhen-2021/tree/main/gesture_detector</a>. </p><p>The model is trained on <a href="https://chalearnlap.cvc.uab.cat/dataset/22/description/">Continuous Gesture Recognition dataset</a>. The model is trained on tesla V100 with 500 epochs and it takes 7 hours. </p><p>I created also a container with Anaconda environment in it. The definition file can be found here: <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/blob/main/singularity/ubuntu20.def">https://github.com/YunfeiZHAO/gsoc-redhen-2021/blob/main/singularity/ubuntu20.def</a>. You can follow my <a href="/2021/05/30/GSOC1-singularity/">post</a> of the first week’s GSOC report to create the same container.</p><p>To make a prediction, we need to have the json file of keypoints generated by Openpose. These keypoints will be the input of our model. The model will make 10 predictions for each video. We set the number of prediction to 10 because a small clip of video normally only have 2 or three gestures. You can run <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/blob/main/gesture_detector/evaluate.py">evaluate.py</a> and it will generate the final results with labeled frames and a labeled video.</p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>In this part, I am going to show you some results of differnet steps of my pipeline.</p><ol><li>Output of the selected frames in a video<br><img src="/img/GSOC_2021/report/origin.gif" alt="origin"></li><li>Output of the Openpose on that video<br><img src="/img/GSOC_2021/report/keypoints.gif" alt="origin"></li><li>Output of the gesture detector<br><img src="/img/GSOC_2021/report/results.gif" alt="origin"></li></ol><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>During 2 months of work, I built a very simple pipeline for “Gesture temporal detection pipeline for news videos”, But there is still quite a lot of work to be accomplished. Firstly, I need to make the model works on a long video with multi-person. Then I need to finish the statistical result evaluation with some matrix based on Jaccard distance. The precision of my model needs to be improved.<br>This work is just the start of my journey in Redhen lab. And thanks to Redhen lab and my mentors to give me this opportunity to work on this interesting project. Thanks for your patience and your guide during these two months. This will help me a lot for my further work on this project.</p><p>To be continued!!!</p><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    <categories>
      
      <category>GSOC 2021 Redhen</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>GSOC week5 - Schema of the project and a conversation with a linguist - (Juin 21 ~ Juin 27, 2021)</title>
    <link href="/2021/06/23/cs/GSOC5-schema-conversation-with-linguists/"/>
    <url>/2021/06/23/cs/GSOC5-schema-conversation-with-linguists/</url>
    
    <content type="html"><![CDATA[<h1 id="Blog-Resume"><a href="#Blog-Resume" class="headerlink" title="Blog Resume"></a>Blog Resume</h1><p>Coding period with Redhen during Google Summer of Code 2021. (Juin 21 ~ Juin 27, 2021)</p><ul><li>Resume of the fourth meeting.</li><li>The illustration of the first milestone model.</li></ul><h1 id="Meetings"><a href="#Meetings" class="headerlink" title="Meetings"></a>Meetings</h1><p>On Wednesday 23st Juin, I had our regular meeting with my mentors <a href="https://dbis.dmi.unibas.ch/team/mahnaz-parian-scherb/">Mahnaz Parian-Scherb</a> and <a href="https://sites.google.com/site/cristobalpagancanovas/">Cristóbal Pagán Cánovas</a>. Cristóbal Pagán Cánovas is a linguist and he gives us a very prefessional view of the project and his advice.</p><p>During this meeting, we reach a consensus that the temporal location of gestures (The start point and the endpoint of a gesture) is more meaningful than the classification of a gesture in the perspective of linguistic research.</p><p>Why gestures’ labels are not the key to our project? Computer engineers always want to model the classification of gesture in the same way as other classification problems such as car classification, object classification, etc. As for the detection problem, computer engineers tend to model gesture detection the same way as detecting a car on the road. But gesture detection is more like finding a needle in a haystack. Firstly, there are too many gestures that have the same linguistic meaning. People from different countries with different cultural backgrounds in different circumstances will use different gestures to express the same thing. Secondly, the same gesture may also have different meanings. For example, when we speak, the same phrase with different tones and different speaker’s emotion represents completely different meaning. So the analysis part of a gesture is a very challenging task for linguists.</p><p>What do computer engineers should do, and what this project’s value? To understand human behaviors and gesture meaning, a computer engineer may use his or her professional skill to provide linguists with indicators and statistical analysis of data. For instance, where the gesture starts and where the gesture ends. The probability of the existence of a gesture in this detected segmentation in a video.  The influence of a big movement of a hand key point to a certain gesture to occur.</p><p>Based on the ideas of this meeting, I am going to make a diagram of my model and a foreseeable estimation of its final results.</p><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    <categories>
      
      <category>GSOC 2021 Redhen</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Gesture temperal detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GSOC week2 - Transfer Learning on DETR - (May 31 ~ Juin 06, 2021)</title>
    <link href="/2021/06/06/cs/GSOC2-Transfer-Learning/"/>
    <url>/2021/06/06/cs/GSOC2-Transfer-Learning/</url>
    
    <content type="html"><![CDATA[<h1 id="Blog-Resume"><a href="#Blog-Resume" class="headerlink" title="Blog Resume"></a>Blog Resume</h1><p>Community bonding period with Redhen during Google Summer of Code 2021. (May 31 ~ Juin 06, 2021)</p><ul><li>Resume of first three meetings</li><li>Transfer Learning of DETR.</li></ul><h1 id="Meetings"><a href="#Meetings" class="headerlink" title="Meetings"></a>Meetings</h1><p>On Friday 21st May, I had my first meeting with three of my six mentors. They are <a href="https://dbis.dmi.unibas.ch/team/mahnaz-parian-scherb/">Mahnaz Parian-Scherb</a> who recently finished her PHD degree at the University of Basel of majored in Multimedia Information Retrieval, and she is the mentor who will follow the whole project this summer. She helped me a lot during the preparatio of my GSOC proposal, <a href="https://sites.google.com/view/danielalcaraz">Daniel Alcaraz Carrión</a> who is a cognitive linguist specialises in multimodal communication and <a href="https://sites.google.com/site/inesolza/home">Inés Olza</a> who is a Tenured Researcher in Linguistics. It was a short meeting with self-presentations and a little presentation of my project. They confirmed that my project has a foreseeable value in multimodal communication as we need to retrieve information like hand gestures in huge volume of videos to do further research.</p><p>On Friday 28st May, The whole Redhen Lab held a very lively team meeting. 58 people participated in this meeting, where <a href="https://www.redhenlab.org/summer-of-code/red-hen-lab-gsoc-2021-projects">12 selected students</a> in this year’s program got to know each other and shared our projects. I am very glad to meet so many professors and students who have the same passion to meet here thanks to Redhen lab.</p><h1 id="Introduction-of-DETR"><a href="#Introduction-of-DETR" class="headerlink" title="Introduction of DETR"></a>Introduction of DETR</h1><p>I wrote two blogs about <a href="/2021/03/31/attention/">attention mechnism</a> and <a href="/2021/04/04/DETR/">DETR</a> respectively. If these two terms are new for you, feel free to have a look at them by clicking the links. In conclusion, models based on Transformer are very good tools to extract both local information that represents the shape of the hand in one video frame and global information that represent the motions across several video frames.</p><h1 id="Transfer-Learning-on-DETR"><a href="#Transfer-Learning-on-DETR" class="headerlink" title="Transfer Learning on DETR"></a>Transfer Learning on DETR</h1><p>During this week’s work, I read thoroughly <a href="https://github.com/facebookresearch/detr">the official implementation of DETR</a> published by Facebook. This project uses Python with Pytorch as the machine learning framework hat makes it easy to use and easy to understand.</p><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    <categories>
      
      <category>GSOC 2021 Redhen</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GSOC week1 - Singularity - (May 24, 2021 - May 30, 2021)</title>
    <link href="/2021/05/30/cs/GSOC1-singularity/"/>
    <url>/2021/05/30/cs/GSOC1-singularity/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction-of-Singularity"><a href="#Introduction-of-Singularity" class="headerlink" title="Introduction of Singularity"></a>Introduction of Singularity</h1><p>During GSOC 2021 with Redhen, We have access to The High Performance Computing Clusters located at <a href="https://case.edu/utech/departments/research-computing">“CWRU”</a> that represents Case Western Reserve University. We need to have our final result run in a Singularity container on this HPC. I used Docker before, and this time I want to make an introduction to Singularity.<br>Before starting reading this article, I will recommand you with two resouces.<br><a href="https://github.com/hpcng/singularity">“Singularity Github”</a><br><a href="https://www.youtube.com/watch?v=vEjLuX0ClN0">“Introduction from San Diego Supercomputer Center”</a></p><ol><li><p>What is a container?<br>A (software) container is an abstraction for a set of technologies that aim to solve the problem of how to get software to run reliably when move from one computing environment to another.</p></li><li><p>What is Singularity?<br>Singularity is a kind of container for HPC (High Performance Computing Clusters).</p></li><li><p>What can Singularity do for you?<br>It’s a docker for HPC, that means it can do all the things that a container can do and you can let no trust users run untrusted containers. In the meanwhile, It supports HPC hardware and scientific applications.</p></li></ol><h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><p align="center"><a><img src="/img/singularity/shipping.png"style="width:600px;">Containers in digital world</a></p>Like what we ship goods with a container by ship, we put a huge variaty of goods in containers and ship them between warehouses, without container, it is less secured and will be a mess during the transport. As different servers or machines have different standards and different environment, we need to use container to make our applications run across different platform.<h2 id="Component-of-Singularity"><a href="#Component-of-Singularity" class="headerlink" title="Component of Singularity"></a>Component of Singularity</h2><ol><li><p>Container Image: A file (or collection of files) saved on disk that stores everything you need to run a taget application or applications: code, runtime, system tools, libraries, etc.</p></li><li><p>Container Process: A container process is simply a standard(Linux) process running on top of the underlying host’s operating system and kernel, but whose software environment is defined by the contents of the container image.</p></li></ol><h2 id="Compare-to-Virtual-Machines"><a href="#Compare-to-Virtual-Machines" class="headerlink" title="Compare to Virtual Machines"></a>Compare to Virtual Machines</h2><p>Container-based applications have direct access to the host kernel and hardware and, thus, are able to achieve similar performance to native applications. In contrast, VM-based applications only have indirect access via the guest OS and hypervisor, which creates a significant performance overhead.</p><p align="center"><a href="https://www.weave.works/blog/a-practical-guide-to-choosing-between-docker-containers-and-vms"><img src="/img/singularity/containers-vs-virtual-machines.jpg"style="width:600px;">Illustration of the structures of container and virtual machine. </a></p><h2 id="Compare-to-Docker"><a href="#Compare-to-Docker" class="headerlink" title="Compare to Docker"></a>Compare to Docker</h2><ol><li><p>HPC systems are shared resources and Docker is not designed for it.</p></li><li><p>Docker’s security model is designed to support trusted users running trusted containers(e.g. Users can escalate to root). </p></li><li><p>Docker is not designed to support tightly-coupled, highly distributed parallel applications(MPI).</p></li></ol><h2 id="Most-Common-Singularity-Use-Cases"><a href="#Most-Common-Singularity-Use-Cases" class="headerlink" title="Most Common Singularity Use Cases"></a>Most Common Singularity Use Cases</h2><ol><li><p>Building and running applications that require newer system libraries than those are available on host system.</p></li><li><p>Running commercial applications binairies that have specific OS requirements which are not met by host system.</p></li><li><p>Converting Docker containers to Singularity containers.</p></li></ol><h2 id="The-Singularity-Workflow"><a href="#The-Singularity-Workflow" class="headerlink" title="The Singularity Workflow"></a>The Singularity Workflow</h2><ol><li><p><strong>Build</strong> your Singularity containers on a local system where you have root or sudo access; e.g., a personal computer where you have installed Singularity.</p></li><li><p><strong>Transfer</strong> your Singularity containers to the HPC system where you wnat to run them.</p></li><li><p><strong>Run</strong> your Singularity containers on that HPC system.</p></li></ol><h1 id="Setup-A-singularity-on-HPC"><a href="#Setup-A-singularity-on-HPC" class="headerlink" title="Setup A singularity on HPC"></a>Setup A singularity on HPC</h1><p>It is recommanded to install the same version of Singularity used on the HPC system where you plan to run your containers. If you run on multiple HPC systems, then install the lowest version number you expect to use. It is better to use Ubuntu or Fedora-based local system. I chose to use Ubuntu 20.04 for this part.</p><p>Besides building a container there are also two interesting registry for Singularity.</p><ol><li><a href="https://cloud.sylabs.io/home">Sylabs.io HUB</a> (commercialised)</li><li><a href="http://datasets.datalad.org/?dir=/shub">Singularity HUB</a> (oringinal)</li></ol><p>They are like docker hub where you can build, share or download images.</p><h2 id="Useful-documents"><a href="#Useful-documents" class="headerlink" title="Useful documents"></a>Useful documents</h2><p>You may have a look at the detailed instructions to use <a href="https://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container">Singularity  Lab: Build A Container</a>.<br>There is also a very useful document <a href="https://sites.google.com/case.edu/techne-public-site/singularity">on Redhen about using Sigularity</a>.<br>There is also an introduction of using singularity on <a href="https://sites.google.com/a/case.edu/hpcc/hpc-cluster/software/Software-Guide/s-t/singularity">Case Western Reserve HPC</a>.<br>To install Singularity on your computer, have a look on <a href="https://sylabs.io/guides/3.3/user-guide/installation.html">the official documents of Singularity container</a>.</p><h2 id="Build-an-Openpose-singularity-container"><a href="#Build-an-Openpose-singularity-container" class="headerlink" title="Build an Openpose singularity container"></a>Build an Openpose singularity container</h2><ol><li>Write a singularity definition file we name as openpose.def here. I followed steps introduced by Frankier and Zhiqi who are GSOC2020 students. You can have a look at their <a href="https://github.com/frankier/openpose_containers">github</a>. My definition file can be found on <a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021/tree/main/singularity">My github</a>.</li><li>If you want to build Singularity container locally, you need to follow the <a href="https://singularity-tutorial.github.io/01-installation/">introduction here</a>. I chose the Sigularity version 2.5.2 which can aussi be used on my HPC server. If you want to build a container locally, you need to have enough space on your working space, it can easily take more than 1g of memory depends on your project.</li><li>To build a container, you need to have a root access and you can use this command to get into it.<ul><li><strong>sudo -i</strong><br> To run the build process, you need the command like this:</li><li><strong>sudo singularity build --sandbox openpose_container openpose.def</strong></li></ul></li><li>After the building process, you will find a new folder is created named openpose_container. You can shell into the container as follows (-w means “writable):<ul><li><strong>sudo singularity shell -w openpose_container</strong></li></ul></li><li>To create the final portable unchangeable image (after training etc.):<ul><li><strong>sudo singularity build openpose_container.simg openpose_container</strong></li></ul></li><li>Finally, you can either upload the whole folder of openpose_container or openpose_container.simg on your HPC. It is recommended by using <strong>rsync</strong> command than use <strong>scp</strong>. Because you need to upload some very big files only once. If the connection is interrupted, you can continue the work by reusing the same command.<ul><li><strong>sudo rsync --progress -a .&#x2F;openpose_container&#x2F; server_domain_name:~&#x2F;openpose_container&#x2F;</strong><br> Then you can easily run this command on HPC:</li><li><strong>singularity shell --nv --bind .&#x2F;home:&#x2F;home openpose_container&#x2F;</strong></li></ul></li></ol><p>From this step, we use a new container called ubuntu_container.</p><ol start="7"><li><p>If you want to a container to have a shared folder with your host, you can use the following command.</p><ul><li><strong>sudo singularity shell -w --bind ~&#x2F;Desktop&#x2F;container:&#x2F;home  ubuntu_container</strong><br> In this way, you can bind the folder ~&#x2F;Desktop&#x2F;container on your host to &#x2F;home on your container, and with flag <strong>-w</strong> all modifications will resist even you exit the container.</li></ul></li><li><p>To use a user in a container, but you do not in the sudoer group on HPC to change user. You can following these steps:</p><ul><li>Create a home folder locally and mount the home folder on a container home directory when run it with sudo.</li><li>Add a user with the same user name on HPC and switch to this user. Install everything like Anaconda, etc.</li><li>Upload the container(If it is already on the HPC, you do not need to do this) and the home folder on HPC. </li><li>Then, on HPC, you can run the container by binding the home directories like what you do locally, and you will find you are using the user in the container as the same name you have in HPC and the home directory is what you created locally.</li></ul></li><li><p>Load some necessary modules on HPC:</p><ul><li><strong>module load singularity</strong></li><li><strong>module load cuda</strong></li><li><strong>srun -p gpu --gpus 1 --mem 4000 --cpus-per-gpu 2 --pty bash # do -h to know more options</strong></li><li><strong>pestat -p gpu -w <hostname> # this will show memory usage, has to be run on another terminal</strong></li></ul></li><li><p>Use GPU in singularity:</p><ul><li>You can firstly have a look at this document from <a href="https://sylabs.io/guides/3.5/user-guide/gpu.html">Sylab.io</a></li><li>what I do is to use <strong>singularity shell --nv --bind .&#x2F;home:&#x2F;home --bind &#x2F;usr&#x2F;local&#x2F;cuda-11.2:&#x2F;usr&#x2F;local&#x2F;cuda ubuntu_container</strong><br>In this way you mount cuda directory and also a list of files in &#x2F;usr&#x2F;bin of the host to your conatainer:<br>nvidia-cua-mps-control dnvidia-cuda-mps-server nvidia-debugdump nvidia-persistenced nvidia-smi</li><li>You need to add cuda path in your .bashrc and source it<div class="code-wrapper"><pre><code class="hljs CUDA">export PATH=$PATH:/usr/local/cuda/binexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64</code></pre></div></li></ul></li></ol><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    <categories>
      
      <category>GSOC 2021 Redhen</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HPC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GSOC week0 - Homepage</title>
    <link href="/2021/05/23/cs/GSOC0-homepage/"/>
    <url>/2021/05/23/cs/GSOC0-homepage/</url>
    
    <content type="html"><![CDATA[<h1 id="GSOC-2021"><a href="#GSOC-2021" class="headerlink" title="GSOC 2021"></a>GSOC 2021</h1><p>I am very happy to be accepted this year as a GSOC student in the Redhen lab. Google Summer of Code is a global program focused on bringing more student developers into open source software development. Students work with an open-source organization on a 10-week programming project during their break from school. <a href="https://summerofcode.withgoogle.com/organizations/?sp-page=3">The organization list for 2021 is listed here</a> </p><h1 id="Redhen-Lab"><a href="#Redhen-Lab" class="headerlink" title="Redhen Lab"></a>Redhen Lab</h1><p>The International Distributed Little Red Hen Lab™ is a global big data science laboratory and cooperative for research into multimodal communication. The main idea of the Redhen lab is cooperation. It is not like a traditional university where professors lead students to do research, while, in the Redhen Lab everyone makes contributions to make progress of research in the multimodal communication world. <a href="https://www.redhenlab.org/home">The Redhen Lab home page</a> can be found here if you are interested in this community.</p><h1 id="Project-Description"><a href="#Project-Description" class="headerlink" title="Project Description"></a>Project Description</h1><p>Gesture recognition becomes popular in recent years since it can play an essential role in many fields, such as non-verbal communication, emotion analysis, human-computer interaction, etc.  We can notice that people use quite a lot of hand gestures in daily life. The research task is to detect hand gestures in raw news videos that are streams of RGB images. I propose a keypoints-based pose tracking system for human tracking and a Transformer and keypoints-based gesture detector for gesture detection to fulfill this task. This structure is composed of a keypoints extractor, a person tracker, and a gesture detector. So the mission has three main parts. The first part is to track people in temporal space.  In the second part, for each person, we use their hand keypoints features in temporal space to construct several keypoints sequences. The third part is to use these sequences to make predictions of the existence of gestures. I believe that for gesture detection tasks, both spatial and temporal information is important. So that is why we use the Transformer that can take into account the local shape information of hands in one frame and can also capture the global hand motion information across the frames. As hand gestures in news videos do not have a good definition of label class, we start with only detecting the existence of a hand gesture. The classification can be easily extended. The final evaluation will be done on Redhen’s “Newscape Dataset”.</p><h1 id="Weekly-Report"><a href="#Weekly-Report" class="headerlink" title="Weekly Report"></a>Weekly Report</h1><p>    <a style="color:#FF851B ;font-size: 24px;">Community Bonding Period</a>     <a style="float: right;font-weight: bold;">(From May 17, 2021 till June 7, 2021)</a></p><ul><li><a href="/2021/05/23/GSOC0-homepage/">Week 0 Homepage</a> <a style="float: right;">(May 17 ~ May 23, 2021)</a></li><li><a href="/2021/05/30/GSOC1-singularity/">Week 1 Report: Setup Singularity on HPC</a> <a style="float: right;">(May 24 ~ May 30, 2021)</a></li><li><a href="/2021/06/06/GSOC2-Transfer-Learning/">Week 2 Report: Transfer Learning for DETR</a> <a style="float: right;">(May 31 ~ Juin 06, 2021)</a></li></ul><p>    <a style="color:#FF851B ;font-size: 24px;">Coding Period Before the First Evaluation</a>     <a style="float: right;font-weight: bold;">(First evaluation time July 12 - 16, 2021)</a></p><ul><li><a href="">Week 3 Report</a> <a style="float: right;">(Juin 07 ~ Juin 13, 2021)</a></li><li><a href="">Week 4 Report</a> <a style="float: right;">(Juin 14 ~ Juin 20, 2021)</a></li><li><a href="/2021/06/23/GSOC5-schema-conversation-with-linguists/">Week 5 Report</a> <a style="float: right;">(Juin 21 ~ Juin 27, 2021)</a></li><li><a href="">Week 6 Report</a> <a style="float: right;">(Juin 28 ~ July 04, 2021)</a></li><li><a href="">Week 7 Report</a> <a style="float: right;">(July 05 ~ July 11, 2021)</a></li></ul><p>    <a style="color:#FF851B ;font-size: 24px;">Coding Period Before the Final Evaluation</a>     <a style="float: right;font-weight: bold;">(Final evaluation time August 16 - 23, 2021)</a></p><ul><li><a href="">Week 8 Report</a> <a style="float: right;">(July 12 ~ July 18, 2021)</a></li><li><a href="">Week 9 Report</a> <a style="float: right;">(July 19 ~ July 25, 2021)</a></li><li><a href="">Week 10 Report</a> <a style="float: right;">(July 26 ~ August 01, 2021)</a></li><li><a href="">Week 11 Report</a> <a style="float: right;">(August 02 ~ August 08, 2021)</a></li><li><a href="">Week 12 Report</a> <a style="float: right;">(August 09 ~ August 15, 2021)</a></li></ul><p><a href="/2021/08/22/GSOC-final-2021/">Final result</a> <a style="float: right;">(August 31, 2021)</a></p><h1 id="Related-Papers"><a href="#Related-Papers" class="headerlink" title="Related Papers"></a>Related Papers</h1><h2 id="Computer-Vision"><a href="#Computer-Vision" class="headerlink" title="Computer Vision"></a>Computer Vision</h2><p>[1]: Nicolas Carion, Francisco Massa et al. <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a>ECCV 2020.<br>[2]: Zhe Cao, Gines Hidalgo, Tomas Simon,  Shih-En Wei, Yaser Sheikh et al. <a href="https://ieeexplore.ieee.org/document/8765346">“OpenPose: Realtime Multi-Person 2D PoseEstimation using Part Affinity Fields”</a> IEEE Transactions on Pattern Analysis and Machine Intelligence.  Date of publication: 17 July 2019.</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>[1]:  Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick et al. <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf">“AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions”</a> CVPR 2018.</p><h1 id="Links-for-this-project"><a href="#Links-for-this-project" class="headerlink" title="Links for this project"></a>Links for this project</h1><ul><li><a href="https://github.com/YunfeiZHAO/gsoc-redhen-2021">GSOC 2021 Yunfei ZHAO Github</a></li><li><a href="https://www.redhenlab.org/summer-of-code/red-hen-lab-gsoc-2021-projects">Project page on Redhen</a></li><li><a href="https://www.overleaf.com/read/jwnknydxtpdh">Original proposal of this project</a></li></ul><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    <categories>
      
      <category>GSOC 2021 Redhen</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Event</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DETR: DEtection TRansformer</title>
    <link href="/2021/04/04/cs/DETR/"/>
    <url>/2021/04/04/cs/DETR/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Before reading this article, you may have a look at my previous article <sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yunfei ZHAO “Transformer? Attention!”.">[2]</span></a></sup><a href="https://blog.yunfeizhao.com/2021/03/31/attention/">“Transformer? Attention!”</a> in which I explain “Attention mechanism” and “The Transformer”. Early in 2020, Facebook AI proposed a new way to use the Transformer in object detection task in paper <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a>  in which they present a new method that view object detection as a direct set prediction problem.  For now, it only has its basic version that means that it still has quite a lot of potential and can be optimized. However, it has already significantly outperformed competitive baselines for accuracy and run-time. </p><p>There are four special characteristics for this model:</p><ol><li>DETR predicts all objects at once and is trained end-to-end.</li><li>DETR does not have multiple hand-designed components that encode prior knowledge, like sliding windows, spatial anchors, non-maximal suppression, large set of proposals, or window centers.</li><li>DETR does not require any customized layers, and thus can be reproduced easily in any framework that conatins standard CNN and transformer classes.</li><li>DETR can be easily generalized to produce panoptic segmentation in a unified manner.</li></ol><p>Training code and pretrained models are available at <a href="https://github.com/facebookresearch/detr">“End-to-End Object Detection with Transformers”</a>.</p><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/detr_architecture.png" alt="DETR directly predicts (in parallel) the final set of detections by combining a common CNN with a transformer architecture. During training, bipartite matching uniquely assigns predictions with ground truth boxes. Prediction with no match should yield a “no object” (∅) class prediction." style="width:800px;"></p><p>The overall DETR architecture is surprisingly simple. It contains three main components.</p><ol><li>A CNN backbone to extract a compact feature representation.</li><li>An encoder-decoder transformer.</li><li>A simple feed forward network(FFN) that makes the final detection prediction.</li></ol><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p>Starting from the initial image $x_{img}\in R^{3\times H_0\times W_0}$(with 3 color channels) and All imput image are batched together, applying 0-padding adequately to ensure they all have the same dimensions ($H_0, W_0$) as the largest image of the batch. The conventional CNN backbone generats a lower-resolution activation map $f\in R^{C\times H\times W}$ as a <strong>set of image features</strong>. The dimensions of this feature map are $C &#x3D; 2048$ and $H,W &#x3D; \frac{H_0}{frac}, \frac{W_0}{frac}$ where <strong>frac</strong> is 32 for DETR and 16 for DETR DC5.</p><h2 id="Transformer-encoder"><a href="#Transformer-encoder" class="headerlink" title="Transformer encoder"></a>Transformer encoder</h2><p>Each encoder layer has a standard architecture (<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[3]</span></a></sup> <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a>)and consist of a multi-head self-attention module and a feed forward network (FFN). The encoder follows the following steps:</p><ol><li>Use a $1\times1$ convolutional kernal to reduces the channel dimension of the high-level activation map $f$ of dimension $R^{C\times H\times W} $ to a new feature map $z_0$ of dimension $R^{d\times H\times W}$ where $d$ is smaller than $C$.</li><li>Collapse $z_0$ into one dimension, resulting in a $d\times HW$ feature map which is the input sequence for the encoder.</li><li>The transformer architecture is permutation-invariant, we supplement it with fixed <span style="color:red"><strong>positional encodings</strong>  that are added to the input of <strong>each attention layer(how? in this case)</strong>.</span>.</li></ol><h2 id="Transformer-decoder"><a href="#Transformer-decoder" class="headerlink" title="Transformer decoder"></a>Transformer decoder</h2><p>The decoder follows the standard architecture of the transformer too, transforming $N$ embeddings of size $d$ using multi-headed self- and encoder-decoder attention mechanism.  To be noted that this model decodes the $N$ objects in parallel at each decoder layer(not autoregressive). Since the decoder is permutation-invariant, the N input embeddings must be different to produce different results. These input embeddings are <span style="color:red"><strong>learnt positional encodings</strong></span> that we refer to as <em><strong>Object Queries</strong></em>. These queries are added to the input of each attention layer similarly to the encoder.</p><p>It follows the following steps:</p><ol><li>The $N$ object queries are transformed into an output embedding by the decoder.</li><li>The output embeddings are independent decoded into box coordinates and class labels by a <strong>feed forward network</strong>, resulting N final predictions.</li></ol><p>Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using <strong>pair-wise relations between them</strong>, while being able to use <strong>the whole image as context</strong>.</p><h2 id="Prediction-feed-forward-networks-FFNs"><a href="#Prediction-feed-forward-networks-FFNs" class="headerlink" title="Prediction feed-forward networks(FFNs)"></a>Prediction feed-forward networks(FFNs)</h2><p>$$<br>FFN(x) &#x3D; RELU(xW_1 + b_1)W_2 + b_2<br>$$</p><p>The final prediction is computed a 3-layer perceptron with RELU activation function and hidden dimension $d$, and a linear projection layer. Like YOLO, The FFN predicts the normalized center coordinates, height and weidth of the box w.r.t. the input image. It predicts also the class label using a softmax function. An additional special class label $\varnothing$ is introduced and is used to represent that no object is detected within a slot (The “background” class) as we predict a fixed-size set of N bounding box, where N is usually much larger than the actual number of objects of interest in an image.</p><h2 id="Auxiliary-decoding-losses"><a href="#Auxiliary-decoding-losses" class="headerlink" title="Auxiliary decoding losses"></a>Auxiliary decoding losses</h2><p>It is helpful to use auxiliary losses as  <sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rami Al-Rfou “Character-Level Language Modeling with Deeper Self-Attention”  AAAI 2019.">[4]</span></a></sup> <a href="https://ojs.aaai.org//index.php/AAAI/article/view/4182">“Character-Level Language Modeling with Deeper Self-Attention” </a> in decoder during training, espacially to help the model output the correct number of object of each class. The idea is to add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. An additional shared layer-norm is used to normalize the input to the prediction FFNs rom different decoder layer.</p><h2 id="Detailed-model-architecture"><a href="#Detailed-model-architecture" class="headerlink" title="Detailed model architecture"></a>Detailed model architecture</h2><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/detailed_detr_architecture.png" alt="DETR uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class and bounding box) or a “no object” class." style="width:800px;"></p><h3 id="Transformer-architecture"><a href="#Transformer-architecture" class="headerlink" title="Transformer architecture"></a>Transformer architecture</h3><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/transformer.png" alt="Architecture of DETR’s transformer.." style="width:800px;"></p><h1 id="Set-Prediction-and-Loss-function"><a href="#Set-Prediction-and-Loss-function" class="headerlink" title="Set Prediction and Loss function"></a>Set Prediction and Loss function</h1><p>As we mentioned at the beginning, this model views object detection as a direct set prediction problem.The basic set prediction task is mutilabel classification and is does not apply to problem such as detection where there is an underlying structure between elements(i.e., near-identical boxes which need post-processing like NMS to eliminate duplicates). So we designed a loss based on the Hungarian algorithm to find a biopartite matching between ground-truth and prediction. In this way, it enforces permutation-invariance, and guarantees that each target element has a unique match by making the model to learning the underlying structure between elements and makes the pipeline post-processing free. To ensure the performance, we use also <span style="color:red">parallel decoding</span> for the transformer.</p><p>The key element for training prossess is the <strong>loss function</strong>. DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger thatn the typical number of object in an image and we need to score predicted objects (<strong>class, position, size</strong>) with respect to the ground truth. So the loss function following two steps:</p><ol><li>Produce an optimal bipartite matching between predicted and ground truth objects.</li><li>Optimize object-specific (bounding box) losses.</li></ol><h2 id="Search-for-the-permutation"><a href="#Search-for-the-permutation" class="headerlink" title="Search for the permutation"></a>Search for the permutation</h2><p>This part, we will try to find the optimal permutation of $N$ predicted elements $\hat{\sigma} \in \mathcal{G}$. $y$ is the ground truth set of objects, and $\hat{y} &#x3D; \{\hat{y}_i \}_{i&#x3D;1}^{N}$.<br>Given that the size of y is smaller than $N$, it will be padded with $\varnothing$.</p><p>$$<br>\hat{\sigma} &#x3D; \arg min_{\sigma \in \mathcal{G}_N} \sum_{i}^{N}\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)}),<br>$$</p><p>where $\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)})$ is a paire-wise <em>matching cost</em> between ground truth $y_i$ and a prediction with index $\sigma(i)$. This optimal assignment is computed efficiently with the Hungarian algorithm, following prior work (e.g. <sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Russell Stewart et al. “End-to-end people detection in crowded scenes” CVPR 2016">[5]</span></a></sup><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Stewart_End-To-End_People_Detection_CVPR_2016_paper.pdf">“End-to-end people detection in crowded scenes”</a>).</p><h3 id="Matching-cost"><a href="#Matching-cost" class="headerlink" title="Matching cost"></a>Matching cost</h3><p>The matching cost takes into account:</p><ol><li>The class prediction. We define $c_i$ as the target class label of the ith ground truth.</li><li>The similarity of predicted and ground truth boxes. We define $b_i \in  [0 ,1]^4$ as a vector that defines ground truth box center coordinates and its heigt and width relative to the image size.</li></ol><p>Each element i of the ground truth set can be seen as a $y_i &#x3D; (c_i, b_i)$. For the prediction with the permutation $\sigma(i)$ we define probability of class $c_i$ as $\hat{p}_{\sigma(i)}(c_i)$ and the predicted box as $\hat{b}_{\sigma(i)}$. We define :</p><p>$$<br>\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)}) &#x3D;<br>\underbrace{-\unicode{x1D7D9}_{\{c_i\neq\varnothing\}}\hat{p}_{\sigma(i)}(c_i)}_{\text{Higher probability of right class}} +<br>\underbrace{\unicode{x1D7D9}_{\{c_i\neq\varnothing\}}\mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)})}_{\text{Closer bounding box}}<br>$$<br>By using the permutation, we assume that we are finding <span style="color:red">one-to-one matching for direct set prediction without duplicates</span>.<br>In the matching cost we use probabilities $\hat{p}_{\hat{\sigma}(i)}(c_i)$ instead of log-probabilities. This makes the class prediction term commensurable to $\mathcal{L}_{box}(.,.)$. </p><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>After finding our optimal permutation $\hat{\sigma}$ in the last procedure, we compute the loss function in this step. The loss function is defined as a linear combination of a negative log-likelihood for class prediction and a box loss defined later:</p><p>$$<br>\mathcal{L}_{Hungarian}(y, \hat{y}) &#x3D; \sum_{i&#x3D;1}^{N}\left[<br>    -log\hat{p}_{\hat{\sigma}(i)}(c_i) +<br>    \unicode{x1D7D9}_{\{c_i\neq\varnothing\}}\mathcal{L}_{box}(b_i, \hat{b}_{\hat{\sigma}(i)})<br>    \right],<br>$$</p><p>To be noted that during the procedure of finding the permutation, we do not take the class label probability for $c_i &#x3D; \varnothing$ into account, while for this loss function, we take it into account. In practice, we down-weight the log-probability term when $c_i &#x3D; \varnothing$ by a factor 10 to account for class imbalance.</p><h2 id="Bounding-box-loss"><a href="#Bounding-box-loss" class="headerlink" title="Bounding box loss"></a>Bounding box loss</h2><p>To mitigate the scales problem of $l_1$ loss which induce the inconsistency between big object and little object, a linear combination of the &amp;l_1&amp; loss and the generalized $IoU$ loss is used.</p><p>$$<br>L_{box}(b_i, \hat{b}_{\sigma(i)}) &#x3D; \lambda_{iou}L_{iou}(b_i, \hat{b}_{\sigma(i)}) +<br>\lambda_{L1}||b_i - \hat{b}_{\sigma(i)}||_1<br>$$</p><p>where $\lambda_{iou}$, $\lambda_{L1} \in \mathcal{R}$ are hyperparameters. These two losses are normalized by the number of objects inside the batch.</p><h1 id="DETR-for-panoptic-segmentation"><a href="#DETR-for-panoptic-segmentation" class="headerlink" title="DETR for panoptic segmentation."></a>DETR for panoptic segmentation.</h1><p>DETR can be naturally extend by adding a mask head on top of the decoder outputs for panoptic segmentation. This head can be used to produce panoptic segmentation by treating stuff and thing classes in a unified way.</p><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nicolas Carion, Francisco Massa et al. “End-to-End Object Detection with Transformers”ECCV 2020.">[1]</span></a></sup> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a></p><p align="center"><img src="/img/DETR/panoptic_head.png" alt="Illustration of the panoptic head. A binary mask is generated in parallel for each detected object, then the masks are merged using pixel-wise argmax." style="width:800px;"></p><p>To DETR panoptic segmentation has 4 parts:</p><ol><li>We train DETR to predict boxes around both $stuff$ and $things$ classes on COCO, using the same recipe. (Hungarian matching is computed using distance between boxes, so predicting boexs is required for the training to be possible)</li><li>A mask head predicts a binary mask for each of the predicted boxes. It takes as input the output of the transformer decoder for each object computes mutli-head(with M heads) attention scores of this embedding over the output of the encoder, generating M attention heatmaps per object in a small resolution.</li><li>An FPN-like architecture is used to make the final prediction and increase the resolution. The final resolution of the masks has stride 4 and each mask is supervised independently using the DICE&#x2F;F-1 loss and Focal loss.</li><li>We predict the final panoptic segmentation we simply use an argmax over the mask scores at each pixel, and assign the corresponding categories to the resulting masks.</li></ol><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>DETR is a new design for object detection systems based on Transformer and bipartite matching loss from direct set prediction. It is one of the most popular model in 2020. Without bells and whistles, it achieves comparable results to a well optimized Faster R-CNN baseline on the challenging COCO dataset.<br>This new design for detectors also comes with new challenges, in particular regarding training, optimization and performances on small objects and it has large potential to be optimized.</p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><div class="code-wrapper"><pre><code class="hljs dust"><span class="language-xml">@article</span><span class="hljs-template-variable">&#123;zhao_DETR2021, </span><span class="hljs-template-variable">    title=&#123;DETR: DEtection TRansformer&#125;</span><span class="language-xml">, </span><span class="language-xml">    url=</span><span class="hljs-template-variable">&#123;https://blog.yunfeizhao.com/2021/04/04/DETR/&#125;</span><span class="language-xml">, </span><span class="language-xml">    journal=</span><span class="hljs-template-variable">&#123;https://blog.yunfeizhao.com/&#125;</span><span class="language-xml">, </span><span class="language-xml">    author=</span><span class="hljs-template-variable">&#123;ZHAO, Yunfei&#125;</span><span class="language-xml">, year=</span><span class="hljs-template-variable">&#123;2021&#125;</span><span class="language-xml">, month=</span><span class="hljs-template-variable">&#123;Mar&#125;</span><span class="language-xml">&#125;</span></code></pre></div><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Nicolas Carion, Francisco Massa et al. <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a>ECCV 2020.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Yunfei ZHAO <a href="https://blog.yunfeizhao.com/2021/03/31/attention/">“Transformer? Attention!”</a>.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Ashish Vaswani, et al. <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a> NIPS 2017.<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Rami Al-Rfou <a href="https://ojs.aaai.org//index.php/AAAI/article/view/4182">“Character-Level Language Modeling with Deeper Self-Attention”</a>  AAAI 2019.<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Russell Stewart et al. <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Stewart_End-To-End_People_Detection_CVPR_2016_paper.pdf">“End-to-end people detection in crowded scenes”</a> CVPR 2016<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Computer vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Paper reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer? Attention!</title>
    <link href="/2021/03/31/cs/attention/"/>
    <url>/2021/03/31/cs/attention/</url>
    
    <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Self-attention, it is a mechanism first used for nature language processing, such as language translation and text content summary,etc. Self-attention sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence, the sequence can be a phrase in NPL task.  At the time Google Brain released <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup><a href="https://arxiv.org/abs/1706.03762">“Attention is all you need.”</a>, this mechanism have already become an integral part of compelling sequence modeling and transduction models in various tasks. However such attention mechanism are used in conjunction with a recurrent network(RNN) or a convolutonal network(CNN). Given a sequence of words, if we see the word “eating”, then we will take more attention to the following name of food.</p><p align="center"><img src="/img/attention/sentence-example-attention.png" alt="Attention between words" style="width:400px;height:100px;"></p><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><p>One of the example of the application of this mechanism is the famous <sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ilya Sutskever, et al. “Sequence to Sequence Learning with Neural Networks”NIPS 2014.">[2]</span></a></sup><a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">“Sequence to Sequence Learning with Neural Networks”</a> model. Broadly speaking, it aims to transform an input sequence (source) to a new one (target) and both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages in either text or audio, and for tasks like adding video caption is also possible. The main struture Seq2seq model uses is the <strong>Encoder-Decoder</strong> structure.</p><p align="center"><img src="/img/attention/encoder-decoder.png" alt="RNN Encoder Decoder structure" style="width:400px"></p><ul><li><strong>Encoder</strong> processes the input sequence and compresses the information into a context vector of a fixed length. This vector is expected to be the encoded information or meaning of the input sequence.</li><li><strong>Decoder</strong> is initialized with the context vector to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state.<br>Both the encoder and decoder are recurrent neural networks, i.e. using LSTM or GRU units.<br><strong>disadvantages</strong> While, there are some disadvantages of RNN for this task. Fixed-length context vector design is incapability of remembering long sentences. And for RNN, it has forgetten the fisrt parts once it completes processing the whole input. It also hard to parallel the whole processes, as the layers is run in sequence.</li></ul><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>People try to use CNN to solve the parallel issue.<br><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a></p><p align="center"><img src="/img/attention/CNN.png" alt="Using CNN to replace RNN and CNN can be parallel, Filters in higher layer canconsider longer sequence." style="width:300px"></p>But we have to build a lot more filter to have large receptive field.<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>The Transformer is a model proposed in <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup> based entirely on attention and almost all tasks use a RNN to perform attention mechanism can be repaced by the Transformer. The transformer as the new stat-of-art in 2020 outperforms the Google’s Neural Machine Translation System <sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yonghui Wu et al. “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”CoRR 2016">[5]</span></a></sup><a href="http://arxiv.org/abs/1609.08144">“GNMT”</a> in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering.</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>The Transformer also take the encode-decoder structure. A high level structure is showed below:<br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/The_transformer_encoders_decoders.png" alt="THe encode-decoder structure of the Transformer." style="width:400px"></p><p>The encoder maps an input sequence of sybol representations $x &#x3D; (x_1,..,x_3)$ to a sequence of continous representations $z &#x3D; (z_1,…,z_n)$. Given $z$, the decoder then generate an output sequence $y &#x3D; (y_1,…,y_3)$ of symbols <strong>one element at a time</strong>. At each step the model is auto-regressive, <strong>consuming the previously generated symbols</strong> as additional input when generating the next. This will be represented in <strong>masking process</strong> in the decoder. </p><p align="center"><img src="/img/attention/embeddings.png" alt="Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes." style="width:400px"></p><p>While a more detailed model architecture is represented in “Attention is all you need”<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup> as below:</p><p align="center"><img src="/img/attention/model_architecture.png" alt="The Transformer - model architecture. The encoder and decoder shown in the left and right halves respectively. They both use stacked self-attention and point-wise, fully connected layers." style="width:400px"></p><ul><li><p><strong>The Encoder</strong> is composed of a tack of N&#x3D;6 identical layers. For each layer, it has:</p><ol><li>A multi-head self-attention mechanism sub-layer.</li><li>A position-wise fully connected feed-forward network sub-layer.</li></ol></li><li><p><strong>The Decoder</strong> is also composed of a stack of N&#x3D;6 identical layers. For each layer, it has:</p><ol><li>A multi-head self-attention mechanism sub-layer. The <strong>masking proceess</strong> I talk about before is added here too. Combined with fact that the output embeddings are offset by one position, ensure that the predictions for position $i$ can depend only on the known outputs at positions less than i.</li><li>A <strong>new</strong> multi-head self-attention machanism sub-layer, which performs attention over <strong>the output of the  encoder stack</strong>.</li><li>A position-wise fully connected feed-forward network sub-layer.</li></ol></li></ul><p>For each sub-layer, it has two operations:</p><ul><li>A residual connection, like in <sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="K. He, X. Zhang, S. Ren and J. Sun “Deep Residual Learning for Image Recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109&#x2F;CVPR.2016.90.">[7]</span></a></sup><a href="https://arxiv.org/abs/1512.03385">“Deep Residual Learning for Image Recognition”</a>.</li><li>A layer normalisation<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jimmy Lei Ba et al. “Layer Normalization”">[8]</span></a></sup><a href="https://arxiv.org/abs/1607.06450">“Layer Normalization”</a></li></ul><p>The output of each sub-layer is <strong>LayerNorm(x + Sublayer(x))</strong>, where Sublayer(x) is the function implemented by the sub-layer itself.</p><p>To facilitate the residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model}&#x3D;512$</p><h2 id="A-detail-analyse-of-decoder"><a href="#A-detail-analyse-of-decoder" class="headerlink" title="A detail analyse of decoder"></a>A detail analyse of decoder</h2><h3 id="How-does-each-step-work"><a href="#How-does-each-step-work" class="headerlink" title="How does each step work"></a>How does each step work</h3><p>I was confused in the concept of a masking process showed in the first sub-layer in the decoder and how does the model is auto-regressive at each step. Let’s have a further look at the functionality of the decoder.</p><p>After finishing the encoding phase, the decoding phase begins. Each step in the decoding phase outputs an element from the output sequence. The number of steps depends on a <strong>special symbol which is generated</strong> and indicate that the decoder has completed its output. </p><p>At the begining, a defaut parameter element which indicate the begin of the output sequence is generate from the output of the encoder and then for each step, the elements from <strong>the previous generated sequence</strong> will be fed into the decoder and that is why we need a mask to let decoder to see only the output at position less than $i$ which is the indice of step (prediction for position $i$).  After the fisrt self-attention layer, the output of this layer will meet the hidden elements generated from the encoder(I will explain these hidden element later in the attention function) and go through the whole decoder stack. The mask process is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.<br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/transformer_decoding.gif" alt="Decoder functionality illustration" style="width:600px"></p><p><a href="https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0">“Illustrated Guide to Transformers- Step by Step Explanation”</a></p><p align="center"><img src="/img/attention/mask_score.png" alt="Adding a look-ahead mask to the scaled scores" style="width:400px"></p><h3 id="Final-linear-layer-and-Softmax-layer"><a href="#Final-linear-layer-and-Softmax-layer" class="headerlink" title="Final linear layer and Softmax layer"></a>Final linear layer and Softmax layer</h3><p>The decoder stack outputs a vector of floats and it is final linear layer and softmax layer to turn them into the sequence of worlds. </p><p>For example, the model knows 10,000 unique English world from its training dataset. The linear layer is then a simple fully connected neural network that takes input as decoder stack’s outputs vector and output a 10,000 dimension vector. The softmax layer turns the values in this vector into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.<br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/transformer_decoder_output_softmax.png" alt="This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word." style="width:600px"></p>For the training process, given the vectors of probabilities for the whole predicted sequence, we can then do the backpropagation for the whole model including the encoder and the decoder.<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>In the context of neural networks, <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention</a> is a technique that mimics cognitive attention. The effect enhances the important parts of the input data and fades out the rest – the thought being that the network should devote more computing power on that small but important part of the data. Which part of the data is more important than others depends on the context and is learned through training data by gradient descent.</p><p>The Transformer has it’s own attention mechanism called “Scaled Dot-Product Attention”. I’m going to introduce this model in the following</p><h2 id="Query-Key-Value"><a href="#Query-Key-Value" class="headerlink" title="Query, Key, Value"></a>Query, Key, Value</h2><p>The basic elements of this model is <strong>k: query</strong>, <strong>k: key</strong> of dimension $d_k$ and <strong>v: value</strong> of dimension $d_v$.<br>The matrix of attention is showed below:<br><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup><a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a><br>$$<br>Attention(Q, K, V) &#x3D; softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>where, the compute attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. Let’s just keep this equation in mind for now.</p><p>The high level view of self-attention Layer is like this:<br><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a></p><p align="center"><img src="/img/attention/self_attention_view.png" alt="The high level view of self-attention model. We have a sequence of input x and a sequence of output b" style="width:300px"></p><p>Let’s have a  deeper view of “Scaled Dot-Product Attention”.</p><p align="center"><img src="/img/attention/attention_schema.png" alt="The schema to compute Scaled Dot-Product Attention and it's Matrix version presentation" style="width:800px"></p> <p>This is the schema to compute the scaled Dot-Product attention given an input sequence. People also often use additive attention that compute the compatibility function using a feed-forward network with a single hidden layer. While dot-product attention is much faster and more space-efficient in pratice. But the dot products grow large in magnitude. Assume that the components of $q$ and $k$ are independant random variable with mean 0 and vriance 1. Then their dot product, $q \cdot k &#x3D; \sum^{d_k}_{i&#x3D;1}$, has mean 0 and variance $d_k$. So we have the term $\sqrt{d_k}$ to scale the dot product.</p><h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p>From the paper of the Transformer, they found that it benecial to linearly project the queries, keys, and values h times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. It generate h $d_v$-dimensional output values in parallel and these are concatenated and once again projected, resulting in the final values.<br>$$<br>MultiHead(Q, K, V) &#x3D; Concat(head_1, …, head_h)W^o<br>$$<br>where<br>$$<br>\text{head}_{i}  &#x3D; \text{Attention}(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})<br>$$</p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><p><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a></p><p align="center"><img src="/img/attention/multi_head.png" alt="The schema of multi-head Scaled Dot-Product Attention" style="width:800px"></p><h1 id="Other-main-parts"><a href="#Other-main-parts" class="headerlink" title="Other main parts"></a>Other main parts</h1><p>From the Transformer <a href="/img/attention/model_architecture.png">model architecture schema</a>, we can notice that there are also several other components and I am going to do an introduction to these parts this section.</p><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>Position-wise Feed-Forward Networks is named <strong>Feed Forward</strong> in the model architecture schema and it is included in each layer of encoder and decoder. It is consists of two linear transformations with a RELU activation in between.<br>$$<br>FFN(x) &#x3D; RELU(xW_1 + b_1)W_2 + b_2<br>$$<br>This function is applied to each position separately and identically, another way of describing this is as two convolutions with kernal size 1. While the linear transfermations are the same across different positions,they use different parameters from layer to layer(different $W$ and $b$).</p><h2 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h2><p>Embeddings are learned to convert the input tokens and output tokens to vectors of dimension $d_{model}$. Two embedding layer in encoder, decoder and the pre-softmax linear transformation at the output of the final decoder stack share the same weight matrix. They also multiply those weights by $\sqrt{d_{model}}$</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>We can notice that there is <strong>no positional information</strong> in self-attention as it is composed of dot product operations in parallel for each element of different position in the sequence. Some information about the relative or absolute position of the tokens in sequence need to be injected. The authors of The Transformer<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup> add <strong>“positional encodings”</strong> to the input embeddings at the bottoms of the encoder and decoder stacks.The positional encoding have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. While, there are many other choices of positional encodings, learned and fixed.</p><p>An intuitive way to understand the sum operation between vector of position encoding and the vector of embedded input showed below.<br><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hung-yi Lee. “Transformer”">[3]</span></a></sup><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a><br><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jay Alammar“The Illustrated Transformer”">[6]</span></a></sup><a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a></p><p align="center"><img src="/img/attention/positional_embedding.png" alt="Positional embedding illustration" style="width:800px"></p>By summing these two terms, we integrate the one-hot vector which contains the current position information into the input.<h1 id="Why-Transformer-is-good"><a href="#Why-Transformer-is-good" class="headerlink" title="Why Transformer is good"></a>Why Transformer is good</h1><p>The reasons why Transformer becomes the stat-of-art in 2017 and has appealed attention in AI recently is listed below:</p><ol><li>The relative low computational complexity per layer.</li><li>The large amount of computation that can be parallelized.</li><li>The path length between long-range dependencies in the networl. As we can see that during the generation of Attention, an input element need to dot product with all other inputs and than generate weights and finally gets its attention element by the weighted average on the value of other input elements. It keeps the local and global information.</li></ol><h1 id="Some-interesting-images-from-the-paper"><a href="#Some-interesting-images-from-the-paper" class="headerlink" title="Some interesting images from the paper"></a>Some interesting images from the paper</h1><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ashish Vaswani, et al. “Attention is all you need” NIPS 2017.">[1]</span></a></sup><a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a></p><p align="center"><img src="/img/attention/im1.png" style="width:600px"></p><p align="center"><img src="/img/attention/im2.png" style="width:600px"></p><p align="center"><img src="/img/attention/im3.png" style="width:600px"></p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><div class="code-wrapper"><pre><code class="hljs dust"><span class="language-xml">@article</span><span class="hljs-template-variable">&#123;zhao_Attention2021, </span><span class="hljs-template-variable">    title=&#123;Transformer? Attention!&#125;</span><span class="language-xml">, </span><span class="language-xml">    url=</span><span class="hljs-template-variable">&#123;https://blog.yunfeizhao.com/2021/03/31/attention/&#125;</span><span class="language-xml">, </span><span class="language-xml">    journal=</span><span class="hljs-template-variable">&#123;https://blog.yunfeizhao.com/&#125;</span><span class="language-xml">, </span><span class="language-xml">    author=</span><span class="hljs-template-variable">&#123;ZHAO, Yunfei&#125;</span><span class="language-xml">, year=</span><span class="hljs-template-variable">&#123;2021&#125;</span><span class="language-xml">, month=</span><span class="hljs-template-variable">&#123;Mar&#125;</span><span class="language-xml">&#125;</span></code></pre></div><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Ashish Vaswani, et al. <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a> NIPS 2017.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Ilya Sutskever, et al. <a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">“Sequence to Sequence Learning with Neural Networks”</a>NIPS 2014.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Hung-yi Lee. <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf">“Transformer”</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Nicolas Carion, Francisco Massa et al. <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">“End-to-End Object Detection with Transformers”</a>ECCV 2020.<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Yonghui Wu et al. <a href="http://arxiv.org/abs/1609.08144">“Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”</a>CoRR 2016<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Jay Alammar<a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a><a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>K. He, X. Zhang, S. Ren and J. Sun <a href="https://arxiv.org/abs/1512.03385">“Deep Residual Learning for Image Recognition,”</a> 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109&#x2F;CVPR.2016.90.<a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Jimmy Lei Ba et al. <a href="https://arxiv.org/abs/1607.06450">“Layer Normalization”</a><a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Michael Phi <a href="https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522b">“Illustrated Guide to Transformers- Step by Step Explanation”</a><a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Computer vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Paper reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Some basic sorting algorithms</title>
    <link href="/2020/08/29/cs/sorting-algorithms/"/>
    <url>/2020/08/29/cs/sorting-algorithms/</url>
    
    <content type="html"><![CDATA[<iframe src="http://gadget_pocket.yunfeizhao.com/donation_unit" style="overflow-x:hidden;overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;"  frameborder="0" scrolling="no" allowtransparency="true"></iframe>]]></content>
    
    
    
    <tags>
      
      <tag>Competitive programming</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
